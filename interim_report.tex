%------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

\usepackage[english]{babel} % Language hyphenation and typographical rules

% Document margins
\usepackage[hmarginratio=1:1,top=32mm,left=20mm,right=20mm,columnsep=20pt]{geometry}
% Custom captions under/above floats in tables or figures
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact
\usepackage{textcomp}

% Allows abstract customization
\usepackage{abstract}
% Set the "Abstract" text to bold
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
% Set the abstract itself to small italic text
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{\thetitle}
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{amsmath}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}

\captionsetup[figure]{labelfont={bf},textfont=normalfont}

%------------------------------------------------------------------------------
%   TITLE SECTION
%------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting

\title{MT Interim Report: \\ Neural Word Alignemnt}
\author{%
\textsc{Bailey Parker} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:bailey@jhu.edu}{bailey@jhu.edu}
 \and
 \textsc{Vivian Tsai} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:viv@jhu.edu}{viv@jhu.edu}
 \and
  \textsc{William Watson} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:billwatson@jhu.edu}{billwatson@jhu.edu}
}

\date{}

%------------------------------------------------------------------------------
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\qdist}[1]{\ifmmode\langle#1\rangle\else\textlangle#1\textrangle\fi}
\renewcommand{\vec}[1]{\mathbf{#1}}

\begin{document}

% Print the title
\maketitle

% \section{Introduction}

% TODO

%------------------------------------------------------------------------------

\begin{abstract}
% \noindent \blindtext
We will discuss our intial progress made on our Neural Word Alignemnt. More specifically, we will discuss our data procurement methods and processing algorithms. In addition, we elaborate on our current experimental models, including the Dot Aligner and Bidirectional GRU Aligner. Our loss function, train/eval implmentation, and batching techiques are also discussed.
\end{abstract}

%------------------------------------------------------------------------------
\section{Introduction}

\section{Data Procurement and Processing}
\subsection{Symbol Tokenization}
\subsection{Number Tokenization}
\subsection{Proper Noun Tokenization}
\subsection{Lemmatization Techniques}
\subsection{POS Tagging}

\section{Model Components}
\subsection{Preliminary Notation}
% inputs, lambdas, scaling facotrs, etc, anything on notation
We begin by stating several operations frequently used in our discussion.

\subsubsection{Hadamard Product}

To perform element-wise multiplication of two matricies $A$ and $B$, of equivalent dimensions, we use the hadamard product, defined as the $\circ$.
\begin{equation}
  (A \circ B)_{ij} = A_{ij} \cdot B_{ij}
\end{equation}

\subsubsection{Sigmoid Function}

The sigmoid function $\sigma$ is applied element wise and defined as follows:
\begin{equation}
  \sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}

\subsubsection{Hyperbolic Tangent Function}

The hyperbolic tangent function $\tanh$ is defined as follows:
\begin{equation}
  \tanh(x) = \frac{e^x - e^{-x}} {e^x + e^{-x}}
\end{equation}
and is applied element-wise.

\subsubsection{Variables}
We define our source sequence as $s$, target sequence as $t$, and refer to the $i$-th source token as $s_i$. We refer to the $j$-th target token as $t_j$. We refer to a matrix $\psi$ whose rows represent values related to source tokens $s_i$ and whose columns refer to values realted to target tokens $t_j$. Hence, $\psi$ is an $|s| \times |t|$ sized matrix.

\subsection{Softmax and Log Softmax}
The softmax function transforms a vector of values into a probability distirbution. Appliying the softmax function to an n-dimensional input tensor rescales it so that the elements of the n-dimensional output tensor lie in the range (0,1) and sum to 1.
\begin{equation}
  \sigma(\vec{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}
\end{equation}

For a matrix $\psi$, the rows correspond to source words $s_i$ and the columns correspond to target words $t_j$. In addition, when we softmax with respect to the targets, i.e. $\sigma_t(\psi)$, the softmax is applied per column. If applied per row, we denote this as $\sigma_s(\psi)$.
\begin{equation}
  \sigma_t(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{kj}}}
\end{equation}
\begin{equation}
  \sigma_s(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{ik}}}
\end{equation}

However, it is sometimes better to work in log-space, and use the log softmax operator for numerical stability.
\begin{equation}
  \log \sigma_t(\psi)_{ij} = \log \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{kj}}}
\end{equation}
\begin{equation}
  \log \sigma_s(\psi)_{ij} = \log \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{ik}}}
\end{equation}

\subsection{Word Embeddings}
Word Embedding layers allow for a simple lookup table that stores embeddings of a fixed dictionary and size. More specifically, these layers are often used to store word embeddings and retrieve them using indices. The input to the embedding layer is a list of indices, and the output is the corresponding word embeddings. This allows words to be represented numerically to a set embedding dimension size, and thus can be passed onto later layers.

Word Embeddings can be formulated as a weight matrix $W_e$, where the vector representation of word $w_i$ is the $i$-th row of the matrix, and can be represented as
$W_e[w_i]$.

\begin{equation}
  W_e = \begin{bmatrix}
  \longleftarrow w_1 \longrightarrow \\
  \vdots\\
  \longleftarrow w_i \longrightarrow\\
  \vdots\\
  \longleftarrow w_n \longrightarrow
\end{bmatrix}
\end{equation}

\subsection{Gated Recurrent Units (GRU)}
\label{sec:gru}
Gated Recurrent Units are a more complex formulation to recurrent layers to process sequences, and improve the vanishing gradient problem found in vanilla RNN layers while using less parameters than a Long Short-Term Memory (LSTM) layer\cite{grupaper}. A GRU makes use of 3 gates: $r_t$ reset gate; $z_t$ update gate; $n_t$ new gate. These 3 gates allow for a blending of the hidden state with new input, and are computed as follows for each input $x_t$ in a sequence $\vec{x}$:
\begin{equation}
  \begin{split}\begin{array}{ll}
  r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{t-1} + b_{hr}) \\
  z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{t-1} + b_{hz}) \\
  n_t = \tanh(W_{in} x_t + b_{in} + r_t \circ (W_{hn} h_{t-1}+ b_{hn})) \\
  h_t = (1 - z_t) \circ n_t + z_t \circ h_{t-1} \\
  \end{array}\end{split}
\end{equation}
where $x_t$ is input at time $t$, $h_{t-1}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$, $h_t$ is the new hidden state at time $t$, and $\sigma$ is the sigmoid function.
\subsection{Bidirectional GRU}
\label{sec:bidirectional}

GRUs only process sequences in a forward direction. However, for translation and in general NLP, we also care about the succeeding input. %@viv rephrase
We thus introduce the Bidirectional GRU, which that runs two separate GRU layers in opposite directions and stacks the output s.t. each element has the context of elements before and after it.

Let us define $\operatorname{GRU}(input)$ as a GRU layer that outputs the hidden cells from $input$ on a single forward pass. Let us also define the operator $\overrightarrow{h}$ as the tensor with elements ordered in order $[0,$ $n]$, and $\overleftarrow{h}$ as the tensor elements ordered from $[n,$ $0]$, i.e. reversed.

When flipping the arrow, i.e., from $\overrightarrow{h}$ to $\overleftarrow{h}$, we define this operation as an invert/reverse function, and it flips the elements to the opposite orientation. We additionally declare $\Vert$ as the concatenation operator. Finally, we define $\overrightarrow{h_f}$ as the forward output, $\overleftarrow{h_b}$ as the backward output, and $h_o$ as the final, stacked bidirectional output.

\begin{equation}
  \label{eq:bidirectional}
  \begin{split}
    \begin{array}{ll}
      % fix to have forward and reverse arrows, and update top
      \overrightarrow{h_f} = \operatorname{GRU}(\overrightarrow{input})\\
      \\
      \overleftarrow{h_b} = \operatorname{GRU}(\overleftarrow{input})\\
      \\
      h_o = \overrightarrow{h_f} \,\,\Vert \,\, \overrightarrow{h_b}\\
      \\
    \end{array}
  \end{split}
\end{equation}

From the above equations, we compute the forward output normally; compute the backwards output on the inverted input; and concatenate the forward outputs with the inverted backwards outputs. All final hidden states are provided as well. The output is of size $(N,$ $batch,$ $hidden$ $size*2)$ since we concatenate the outputs of the two GRU layers.


\subsection{Alignment Prior}
\label{sec:alingment_prior}
In order to learn diagonal alignemnts, we describe our formulation of the IBM Model 2 reparameterization as a custom PyTorch layer. We define the prior alignment matrix $A$ as an $|s| \times |t|$. Given a source token $s_i$ and target token $t_j$, and alignment hyperparameter $\lambda$, we deifne the alignment distortion function $a_{ij}$:
\begin{equation}
  a_{ij} = -\lambda | i - j |
\end{equation}

We set the default $\lambda$ value to $4$. This encodes our diagonal prior, and allows us to add in the diagonal weights to our network's weights. We allow for an optional, learnable scaling factor $\alpha$ to control the strength of this prior, defaulted to $0.25$. Hence our IBM Model 2 module outputs the alignemnt prior matrix $A$:
\begin{equation}
  A_{ij} = \alpha \cdot a_{ij}
\end{equation}

\subsection{Batch Matrix Multiplication (BMM)}
\label{sec:bmm}
To combine source and target tensors, we use Batch Matrix Multiplication. Given a matrix $A$ of size $(b,$ $n,$ $m)$ and matrix $B$ of size $(b,$ $m,$ $p)$, we perform matrix multiplication on the sub-matricies to create an output matrix $O$ of size $(b$ $n,$ $p)$.

\begin{equation}
  O_i = A_i \times B_i
\end{equation}

This allows us to combine the vectorized output of our network on each individual sequence and combine them into a matrix tensor of size $(b,$ $|s|,$ $|t|)$. This requires the use of simple transpose functions to align the dimensions of each pairing sentence to correctly compute the BMM.

\section{Model Descriptions}
We use the aformentioned model components to construct complex alignment models, and provide a more through discussion on our techiques.
\subsection{Dot Aligner}
This is our baseline model. This model attempts to learn word embeddings to correctly give alignemnt outputs.

The intuition behind this baseline mdoel was to compute dot products between every source token $s_i$ embedding and target token $t_j$ embedding to generate an alignment matrix via BMM (Section ~\ref{sec:bmm}), combined with the diagonal prior discussed in Section ~\ref{sec:alingment_prior}. The ouput is the alignment matrix $\psi$.

This is a very simple model that only learns the pure embeddings, and therefore we do not expect it to do as well as more complex models. The full computation graph can be seen in Figure ~\ref{fig:dot_aligner}.

\begin{figure}
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (plus) {$+$};
    \node[latent, below=1cm of plus] (psi) {$\psi$};
    \node[rectangle, above=0.75cm of plus, xshift=-2.25cm] (bmm) {BMM};
    \node[rectangle, above=0.5cm of bmm, xshift=-1cm] (embed_s) {Embed};
    \node[rectangle, above=0.5cm of bmm, xshift=1cm] (embed_t) {Embed};
    \node[obs, above=0.75cm of embed_s] (s) {$s$};
    \node[obs, above=0.75cm of embed_t] (t) {$t$};
    \node[obs, right=0.6cm of t] (lamb) {$\lambda$};
    \node[obs, right=0.6cm of lamb] (i) {$i$};
    \node[obs, right=0.6cm of i] (j) {$j$};
    \node[rectangle, below=0.75cm of i] (align) {$a_{ij} = -\lambda |i-j|$};
    % \node[rectangle, below=1cm of align] (logsoft) {$\log \sigma_t(a)$};
    \node[latent, below=0.75cm of align] (mux) {$\times$};
    \node[latent, right=0.5cm of align] (alpha) {$\alpha$};


    % Connect the nodes
    \edge {s} {embed_s};
    \edge {t} {embed_t};
    \edge {embed_s, embed_t} {bmm};
    \edge {lamb, i, j} {align};
    % \edge {align} {logsoft};
    \edge {align, alpha} {mux};
    \edge {mux, bmm} {plus};
    \edge {plus} {psi};

  \end{tikzpicture}
  \caption{Model Architecture for Dot Aligner for source $s$, target $t$ alignments. $\alpha$ is a global learnable scaling factor for the importance of alignment distribution $a$, and $\lambda$ is the global alignment distortion parameter.}
  \label{fig:dot_aligner}
\end{figure}

\subsection{Bidirectional GRU Aligner}

An extension to the capacity of the Dot Aligner is to add a bidirectional GRU after the embedding layers. This will allow the model to consider not only forward propagation of the input sequence, but also the reverse. Therefore, our vectorized encodings of each sequence has the context of the words in its local area for context. The downstream algorithm for forward propagation is the same after the recurrent layers, as detailed in Figure ~\ref{fig:gru_aligner}.

\begin{figure}
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (plus) {$+$};
    \node[latent, below=1cm of plus] (psi) {$\psi$};
    \node[rectangle, above=0.5cm of plus, xshift=-2.25cm] (bmm) {BMM};
    \node[rectangle, above=0.5cm of bmm, xshift=-1cm] (gru_s) {GRU};
    \node[rectangle, above=0.5cm of bmm, xshift=1cm] (gru_t) {GRU};
    \node[rectangle, above=0.5cm of gru_s] (embed_s) {Embed};
    \node[rectangle, above=0.5cm of gru_t] (embed_t) {Embed};
    \node[obs, above=0.75cm of embed_s] (s) {$s$};
    \node[obs, above=0.75cm of embed_t] (t) {$t$};
    \node[obs, right=0.6cm of t] (lamb) {$\lambda$};
    \node[obs, right=0.6cm of lamb] (i) {$i$};
    \node[obs, right=0.6cm of i] (j) {$j$};
    \node[rectangle, below=0.75cm of i] (align) {$a_{ij} = -\lambda |i-j|$};
    % \node[rectangle, below=1cm of align] (logsoft) {$\log \sigma_t(a)$};
    \node[latent, below=1cm of align] (mux) {$\times$};
    \node[latent, right=0.5cm of align] (alpha) {$\alpha$};


    % Connect the nodes
    \edge {s} {embed_s};
    \edge {t} {embed_t};
    \edge {embed_s} {gru_s};
    \edge {embed_t} {gru_t};
    \edge {gru_s, gru_t} {bmm};
    \edge {lamb, i, j} {align};
    % \edge {align} {logsoft};
    \edge {align, alpha} {mux};
    \edge {mux, bmm} {plus};
    \edge {plus} {psi};

  \end{tikzpicture}
  \caption{Model Architecture for Bidirectional GRU Aligner for source $s$, target $t$ alignments. $\alpha$ is a global learnable scaling factor for the importance of alignment distribution $a$, and $\lambda$ is the global alignment distortion parameter.}
  \label{fig:gru_aligner}
\end{figure}

\subsection{Extensions}
Here we discuss possible extensions to the two aformentioned models, to improve the capacity to learn alignments based upon our initial results.

\section{Loss Function}
% quick tldr, point to appendix for each sub thing, jsut describe
% it
\subsection{Supervised Loss}

% get better notation
\begin{equation}
  \phi_{ij} = \begin{cases}
  1 & \text{if } s_i \text{ aligns } t_j \\
  0 & \text{else}
  \end{cases}
\end{equation}

\begin{equation}
  MLE(s, t) = \prod_i \prod_j \left[ \frac{\exp \psi_{ij}}{\sum_k \exp \psi_{ik}} \right]^{\phi_{ij}} \cdot \left[ \frac{\exp \psi_{ij}}{\sum_k \exp \psi_{kj}} \right]^{\phi_{ij}}
\end{equation}

\begin{equation}
  MLE(s, t) = \prod_i \prod_j \left[ \sigma_s(\psi)_{ij} \right]^{\phi_{ij}} \cdot \left[ \sigma_t(\psi)_{ij} \right]^{\phi_{ij}}
\end{equation}

\begin{equation}
  NLL(s, t) = - \sum_i \sum_j  \phi_{ij} \cdot \log \sigma_s (\psi)_{ij} + \phi_{ij} \cdot \log \sigma_t (\psi)_{ij}
\end{equation}

\begin{figure}
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (loss) {$\mathcal{L}$};
    \node[rectangle, above=0.5cm of loss] (nsum) {$-\sum$};
    \node[obs, above=2.5cm of nsum] (mask) {$\phi$};
    \node[obs, above=1cm of mask] (psi) {$\psi$};
    \node[latent, above=0.5cm of nsum, xshift=-0.75cm] (muxs) {$\times$};
    \node[latent, above=0.5cm of nsum, xshift=0.75cm] (muxt) {$\times$};
    \node[rectangle, above=0.5cm of muxs, xshift=-0.75cm] (logs) {$\log \sigma_s (\psi)$};

    \node[rectangle, above=0.5cm of muxt, xshift=0.75cm] (logt) {$\log \sigma_t (\psi)$};

    % Connect the nodes
    \edge {psi} {logs, logt};
    \edge {logs, mask} {muxs};
    \edge {logt, mask} {muxt};
    \edge {muxs, muxt} {nsum};
    \edge {nsum} {loss};

  \end{tikzpicture}
  \caption{Computation graph for our supervised loss function. The generated alignment matrix $\psi$ is compared to the ground truth alignment matrix $\phi$.}
  \label{fig:supervised_loss}
\end{figure}

\subsection{Unsupervised Alignment Loss}

% refrence appendix

Our unsupervised loss function, to be minimized, is a 5-term equation. We define:
\begin{itemize}[label={}]
  \item $a_t$ as the target prior alignment matrix normalized per column with respect to $t$\\ %per col wrt t
  \item $a_s$ as the source prior alignment matrix normalized per row with respect to $s$\\ %per row wrt s
  \item $\sigma_s$ as the softmax operator applied on the rows of a matrix\\
  \item $\sigma_t$ as the softmax operator applied to each column of a matrix
\end{itemize}

Indexing is done under the assumption that source words $s_i$ form rows
and target words $t_j$ form columns. The term $ij$ is a shorthand
for $s_i$ and $t_j$ indexing into our matrices.

\begin{equation}
  \centering
\begin{split}
  Lo&ss = \\
  &- \sum_j^{|t|} \log \left[
      \sum_i^{|s|} \exp \left(
        \log \sigma_s(\theta(t, s))_{ij} + \log \sigma_t(\psi)_{ij} \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_t(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_t(\psi)_{ij}}{a_t(i, j)} \right] \\
  &- \sum_i^{|s|} \log \left[ \sum_j^{|t|}
      \exp \left(
        \log \sigma_t(\theta(t, s))_{ij} + \log \sigma_s(\psi)_{ij}
      \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_s(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_s(\psi)_{ij}}{a_s(i, j)} \right] \\
  &- \log \sum_i^{|s|} \sum_j^{|t|} \left[
    \sigma_s(\psi) \circ \sigma_t(\psi) \right]_{ij} \\
\end{split}
\end{equation}


\section{Train/Eval Implmentation}
\subsection{Vocabulary Building}
\subsection{Batching}
\subsection{Optimizer}
\subsection{Additonal Items}

\section{Ground Truth Alignment Results}
% ?

\section{Current Status}
% what do we currently have for him?

\section{Future Work}
% what to get done by final report



%------------------------------------------------------------------------------

% References
\bibliographystyle{abbrv}
\bibliography{interim_report}

%------------------------------------------------------------------------------


\clearpage
\appendix
\onecolumn

\end{document}

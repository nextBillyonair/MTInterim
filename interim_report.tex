%------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

\usepackage[english]{babel} % Language hyphenation and typographical rules

% Document margins
\usepackage[hmarginratio=1:1,top=32mm,left=20mm,right=20mm,columnsep=20pt]{geometry}
% Custom captions under/above floats in tables or figures
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact
\usepackage{textcomp}

% Allows abstract customization
\usepackage{abstract}
% Set the "Abstract" text to bold
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
% Set the abstract itself to small italic text
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{\thetitle}
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{amsmath}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}

\captionsetup[figure]{labelfont={bf},textfont=normalfont}

%------------------------------------------------------------------------------
%   TITLE SECTION
%------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting

\title{MT Interim Report: \\ Neural Word Alignment}
\author{%
\textsc{Bailey Parker} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:bailey@jhu.edu}{bailey@jhu.edu}
 \and
 \textsc{Vivian Tsai} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:viv@jhu.edu}{viv@jhu.edu}
 \and
  \textsc{William Watson} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:billwatson@jhu.edu}{billwatson@jhu.edu}
}

\date{}

%------------------------------------------------------------------------------
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\qdist}[1]{\ifmmode\langle#1\rangle\else\textlangle#1\textrangle\fi}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newlength\mystoreparindent
\newenvironment{myparindent}[1]{%
  \setlength{\mystoreparindent}{\the\parindent}
  \setlength{\parindent}{#1}
  }{%
  \setlength{\parindent}{\mystoreparindent}
}

\begin{document}

% Print the title
\maketitle

% \section{Introduction}

% TODO

%------------------------------------------------------------------------------

\begin{abstract}
% \noindent \blindtext
We discuss our intial progress on our Neural Word Alignment model. More
specifically, we detail our data procurement methods and processing algorithms;
elaborate on our current experimental models (including the Dot Aligner and
Bidirectional GRU Aligner); and define our loss function, train/evaluation
implementation, and batching techniques.
\end{abstract}

%------------------------------------------------------------------------------
\section{Introduction}
Our neural word alignment model is inspired by IBM Models 1 and 2 (including
Dyer et al. \cite{dyer2013simple}'s reparametrization of the latter); we
replaced the EM algorithm and \verb|fast_align| implementations with a neural
network architecture, using PyTorch. This architecture utilizes gated recurrent
units (GRUs), as is successfully and effectively done in many recent papers
(\cite{bahdanau2014neural}).\footnote{Note that our initial proposal used
long short-term memory (LSTM) layers; however, GRUs prove a better fit in that
they require less parameters and thus counter long training times and the risk
of overfitting (\cite{koehn2019neural}).}

In this paper, we first define relevant equations for our model, then further
describe our architecture and loss function. We then discuss data collection
(as currently completed at this time) and future plans for training/evaluation. 
% @viv

\section{Model Components}
% inputs, lambdas, scaling facotrs, etc, anything on notation
We begin by defining several operations frequently used in our experiments
and discussion.

\subsection{Preliminary Notation}

\subsubsection{Hadamard Product}

To perform element-wise multiplication of two matrices $A$ and $B$ with
equivalent dimensions, we use the Hadamard product, defined as the $\circ$:
\begin{equation}
  (A \circ B)_{ij} = A_{ij} \cdot B_{ij}
\end{equation}

\subsubsection{Sigmoid Function}

The sigmoid function $\sigma$ is applied element-wise and defined as follows:
\begin{equation}
  \sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}

\subsubsection{Hyperbolic Tangent Function}

The hyperbolic tangent function $\tanh$ is defined as follows:
\begin{equation}
  \tanh(x) = \frac{e^x - e^{-x}} {e^x + e^{-x}}
\end{equation}
and is applied element-wise.

\subsubsection{Variables}
We define our source sequence as $s$ and target sequence as $t$ (and
consequently, the $i$-th source token as $s_i$ and the $j$-th target token as
$t_j$).
We also refer to a matrix $\psi$ whose rows represent values related to source
tokens $s_i$ and whose columns refer to values related to target tokens $t_j$.
Hence, $\psi$ is of size $|s| \times |t|$.

\subsection{Softmax and Log Softmax}
The softmax function transforms a vector of values into a probability
distribution. Applying the softmax function to an $n$-dimensional input tensor
rescales it so that the elements of the $n$-dimensional output tensor lie in
range $(0,1)$ and sum to $1$.
\begin{equation}
  \sigma(\vec{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}
\end{equation}

For matrix $\psi$, rows correspond to source words $s_i$ and columns correspond
to target words $t_j$. When we softmax with respect to the target words, i.e.,
the softmax is applied per column and denoted as $\sigma_t(\psi)$
(Equation \ref{eq:softmax-target}); when we softmax with respect to the source
words, the softmax is applied per row and denoted as $\sigma_s(\psi)$
(Equation \ref{eq:softmax-source}).
\begin{equation}
  \label{eq:softmax-target}
  \sigma_t(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{kj}}}
\end{equation}
\begin{equation}
  \label{eq:softmax-source}
  \sigma_s(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{ik}}}
\end{equation}
% reword
While calculating loss, we work in log space and accordingly use the log
softmax operator for numerical stability.
\begin{equation}
  \log \sigma_t(\psi)_{ij} = \log \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{kj}}}
\end{equation}
\begin{equation}
  \log \sigma_s(\psi)_{ij} = \log \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{ik}}}
\end{equation}

\subsection{Word Embeddings}
Word embedding layers allow for a simple lookup table that stores embeddings
of a fixed dictionary and size. More specifically, these layers are often used
to store word embeddings and retrieve them using indices. The input to the
embedding layer is a list of indices, and the output is the corresponding
word embeddings. This allows words to be represented numerically to a set
embedding dimension size, and thus can be passed onto later layers.

Word embeddings can be formulated as a weight matrix $W_e$, where the
vector representation of word $w_i$ is the $i$-th row of the matrix, and
can be represented as $W_e[w_i]$.

\begin{equation}
  W_e = \begin{bmatrix}
  \longleftarrow w_1 \longrightarrow \\
  \vdots\\
  \longleftarrow w_i \longrightarrow\\
  \vdots\\
  \longleftarrow w_n \longrightarrow
\end{bmatrix}
\end{equation}

\subsection{Gated Recurrent Units (GRU)}
\label{sec:gru}

Gated Recurrent Units (GRUs) are a more complex formulation to recurrent layers
to process sequences, and improve the vanishing gradient problem found in
vanilla RNN layers while using less parameters than a Long Short-Term Memory
(LSTM) layer \cite{cho2014learning}.

A GRU makes use of three gates: the $r_t$  reset gate; the $z_t$ update gate;
and the $n_t$ new gate. These allow for a blending of the hidden state with new input. The gates are computed as follows for each input $x_t$ in a sequence
$\vec{x}$:
\begin{equation}
  \begin{split}\begin{array}{ll}
  r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{t-1} + b_{hr}) \\
  z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{t-1} + b_{hz}) \\
  n_t = \tanh(W_{in} x_t + b_{in} + r_t \circ (W_{hn} h_{t-1}+ b_{hn})) \\
  h_t = (1 - z_t) \circ n_t + z_t \circ h_{t-1} \\
  \end{array}\end{split}
\end{equation}
where $x_t$ is input at time $t$, $h_{t-1}$ is the hidden state of the
previous layer at time $t-1$ or the initial hidden state at time $0$, $h_t$
is the new hidden state at time $t$, and $\sigma$ is the sigmoid function.

\subsection{Bidirectional GRU}
\label{sec:bidirectional}

GRUs only process sequences in a forward direction. However, for
translation and in general NLP, we also care about the succeeding input.
%@viv rephrase
We thus introduce the Bidirectional GRU, which that runs two separate GRU
layers in opposite directions and stacks the output such that each element has
the context of elements before and after it.

Let us define $\operatorname{GRU}(input)$ as a GRU layer that outputs the
hidden cells from $input$ on a single forward pass. Let us also define the
operator $\overrightarrow{h}$ as the tensor with elements ordered in
order $[0,$ $n]$, and $\overleftarrow{h}$ as the tensor elements ordered
from $[n,$ $0]$, i.e. reversed.

When flipping the arrow, i.e., from $\overrightarrow{h}$ to
$\overleftarrow{h}$, we define this operation as an invert/reverse
function, and it flips the elements to the opposite orientation. We
additionally declare $\Vert$ as the concatenation operator. Finally, we
define $\overrightarrow{h_f}$ as the forward output, $\overleftarrow{h_b}$
as the backward output, and $h_o$ as the final, stacked bidirectional output.

\begin{equation}
  \label{eq:bidirectional}
  \begin{split}
    \begin{array}{ll}
      % fix to have forward and reverse arrows, and update top
      \overrightarrow{h_f} = \operatorname{GRU}(\overrightarrow{input})\\
      \\
      \overleftarrow{h_b} = \operatorname{GRU}(\overleftarrow{input})\\
      \\
      h_o = \overrightarrow{h_f} \,\,\Vert \,\, \overrightarrow{h_b}\\
      \\
    \end{array}
  \end{split}
\end{equation}

From the above equations, we compute the forward output normally; compute the
backwards output on the inverted input; and concatenate the forward outputs
with the inverted backwards outputs. All final hidden states are provided as
well. The output is of size $(N,$ $batch,$ $hidden$ $size*2)$ since we
concatenate the outputs of the two GRU layers.

\subsection{Alignment Prior}
\label{sec:alignment_prior}
In order to learn diagonal alignments, we describe our formulation of the
IBM Model 2 reparameterization as a custom PyTorch layer. We define the prior
alignment matrix $A$ as an $|s| \times |t|$. Given a source token $s_i$ and
target token $t_j$, and alignment hyperparameter $\lambda$, we define the
alignment distortion function $a_{ij}$:
\begin{equation}
  a_{ij} = -\lambda | i - j |
\end{equation}

We set the default $\lambda$ value to $4$. This encodes our diagonal prior,
and allows us to add in the diagonal weights to our network's weights. We
allow for an optional, learnable scaling factor $\alpha$ to control the
strength of this prior, defaulted to $0.25$. Hence our IBM Model 2 module
outputs the alignemnt prior matrix $A$:
\begin{equation}
  A_{ij} = \alpha \cdot a_{ij}
\end{equation}

\subsection{Batch Matrix Multiplication\\(BMM)}
\label{sec:bmm}
To combine source and target tensors, we use batch matrix multiplication (BMM).
Given a matrix $A$ of size $(b,$ $n,$ $m)$ and matrix $B$ of size
$(b,$ $m,$ $p)$, we perform matrix multiplication on the sub-matrices to
create an output matrix $O$ of size $(b$ $n,$ $p)$.

\begin{equation}
  O_i = A_i \times B_i
\end{equation}

This allows us to combine the vectorized output of our network on each
individual sequence and combine them into a matrix tensor of size
$(b,$ $|s|,$ $|t|)$. This requires the use of simple transpose functions to
align the dimensions of each pairing sentence to correctly compute the BMM.

\section{Model Descriptions}
We use the aformentioned model components to construct complex alignment
models, and provide a more through discussion on our techniques.

\subsection{Dot Aligner}
This is our baseline model. This model attempts to learn word embeddings
to correctly give alignemnt outputs.

The intuition behind this baseline model was to compute dot products
between every source token $s_i$ embedding and target token $t_j$ embedding
to generate an alignment matrix via BMM (Section ~\ref{sec:bmm}), combined
with the diagonal prior discussed in Section ~\ref{sec:alignment_prior}.
The output is the alignment matrix $\psi$.

This is a very simple model that only learns the pure embeddings; we
therefore do not expect it to perform as well as more complex models.
The full computation graph can be seen in Figure ~\ref{fig:dot_aligner}.

\begin{figure}
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (plus) {$+$};
    \node[latent, below=1cm of plus] (psi) {$\psi$};
    \node[rectangle, above=0.75cm of plus, xshift=-2.25cm] (bmm) {BMM};
    \node[rectangle, above=0.5cm of bmm, xshift=-1cm] (embed_s) {Embed};
    \node[rectangle, above=0.5cm of bmm, xshift=1cm] (embed_t) {Embed};
    \node[obs, above=0.75cm of embed_s] (s) {$s$};
    \node[obs, above=0.75cm of embed_t] (t) {$t$};
    \node[obs, right=0.6cm of t] (lamb) {$\lambda$};
    \node[obs, right=0.6cm of lamb] (i) {$i$};
    \node[obs, right=0.6cm of i] (j) {$j$};
    \node[rectangle, below=0.75cm of i] (align) {$a_{ij} = -\lambda |i-j|$};
    % \node[rectangle, below=1cm of align] (logsoft) {$\log \sigma_t(a)$};
    \node[latent, below=0.75cm of align] (mux) {$\times$};
    \node[latent, right=0.5cm of align] (alpha) {$\alpha$};


    % Connect the nodes
    \edge {s} {embed_s};
    \edge {t} {embed_t};
    \edge {embed_s, embed_t} {bmm};
    \edge {lamb, i, j} {align};
    % \edge {align} {logsoft};
    \edge {align, alpha} {mux};
    \edge {mux, bmm} {plus};
    \edge {plus} {psi};

  \end{tikzpicture}
  \caption{Model Architecture for Dot Aligner for source $s$,
  target $t$ alignments. $\alpha$ is a global learnable scaling factor for
  the importance of alignment distribution $a$, and $\lambda$ is the
  global alignment distortion parameter.}
  \label{fig:dot_aligner}
\end{figure}

\subsection{Bidirectional GRU Aligner}

An extension to the capacity of the Dot Aligner is to add a bidirectional GRU
after the embedding layers. This will allow the model to consider not only
forward propagation of the input sequence, but also the reverse. Therefore, our
vectorized encodings of each sequence has the context of the words in its local
area for context. The downstream algorithm for forward propagation is the same
after the recurrent layers, as detailed in Figure ~\ref{fig:gru_aligner}.

\begin{figure}
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (plus) {$+$};
    \node[latent, below=1cm of plus] (psi) {$\psi$};
    \node[rectangle, above=0.5cm of plus, xshift=-2.25cm] (bmm) {BMM};
    \node[rectangle, above=0.5cm of bmm, xshift=-1cm] (gru_s) {GRU};
    \node[rectangle, above=0.5cm of bmm, xshift=1cm] (gru_t) {GRU};
    \node[rectangle, above=0.5cm of gru_s] (embed_s) {Embed};
    \node[rectangle, above=0.5cm of gru_t] (embed_t) {Embed};
    \node[obs, above=0.75cm of embed_s] (s) {$s$};
    \node[obs, above=0.75cm of embed_t] (t) {$t$};
    \node[obs, right=0.6cm of t] (lamb) {$\lambda$};
    \node[obs, right=0.6cm of lamb] (i) {$i$};
    \node[obs, right=0.6cm of i] (j) {$j$};
    \node[rectangle, below=0.75cm of i] (align) {$a_{ij} = -\lambda |i-j|$};
    % \node[rectangle, below=1cm of align] (logsoft) {$\log \sigma_t(a)$};
    \node[latent, below=1cm of align] (mux) {$\times$};
    \node[latent, right=0.5cm of align] (alpha) {$\alpha$};

    % Connect the nodes
    \edge {s} {embed_s};
    \edge {t} {embed_t};
    \edge {embed_s} {gru_s};
    \edge {embed_t} {gru_t};
    \edge {gru_s, gru_t} {bmm};
    \edge {lamb, i, j} {align};
    % \edge {align} {logsoft};
    \edge {align, alpha} {mux};
    \edge {mux, bmm} {plus};
    \edge {plus} {psi};

  \end{tikzpicture}
  \caption{Model Architecture for Bidirectional GRU Aligner for source $s$,
  target $t$ alignments. $\alpha$ is a global learnable scaling factor for the
  importance of alignment distribution $a$, and $\lambda$ is the global
  alignment distortion parameter.}
  \label{fig:gru_aligner}
\end{figure}

\subsection{Extensions}
We discuss possible extensions for the two aformentioned models intended to
improve the capacity for learning alignments (based upon our initial results).

\section{Loss Function}
% quick tldr, point to appendix for each sub thing, just describe
% it
We describe two modes of training: supervised and unsupervised. Supervised
training allows us to measure the capacity of our models to learn given
alignments and is the first step in validating our model's ability to
eventually learn unsupervised alignments.

\subsection{Supervised Loss}
We can formulate our supervised loss as maximizing the probabilities of our
alignments, $\psi$, with the ground truth alignments, represented as $\phi$.

Our targets are binary-valued alignment matrices of size $|s| \times |t|$.
Targets $\phi$ are defined as follows:

\begin{equation}
  \phi_{ij} = \begin{cases}
  1 & \text{if } s_i \text{ aligns with } t_j \\
  0 & \text{else}
  \end{cases}
\end{equation}

\noindent for source token $s_i$ and target token $t_j$.

We now formulate our loss objective as the maximum likelihood estimate (MLE)
between our alignment matrix $\psi$ and target matrix $\phi$. However, $\psi$
are hard weights, and we use the softmax function to convert our alignment
weights to valid probabilities. In addition, we seek to maximize the
probability for both the row ($\sigma_s$) and column ($\sigma_t$) softmax.

Our MLE function to be maximized is then:

\begin{equation}
  MLE(\psi, \phi) = \prod_i \prod_j \left[ \frac{\exp \psi_{ij}}{\sum_k \exp \psi_{ik}} \right]^{\phi_{ij}} \cdot \left[ \frac{\exp \psi_{ij}}{\sum_k \exp \psi_{kj}} \right]^{\phi_{ij}}
\end{equation}

\begin{equation}
  MLE(\psi, \phi) = \prod_i \prod_j \left[ \sigma_s(\psi)_{ij} \right]^{\phi_{ij}} \cdot \left[ \sigma_t(\psi)_{ij} \right]^{\phi_{ij}}
\end{equation}

We can see that maximizing the MLE function allows our network to drive the
probabilities of alignment to be as close to $1$ as possible, and implicity
depresses all other values along the row and column. However, neural networks
do not maximize objective functions, and the multiplication of small
probabilities leads to issues of numerical stability. We therefore define the
following negative log likelihood (NLL) function:

\begin{equation}
  NLL(\psi, \phi) = - \sum_i \sum_j  \phi_{ij} \cdot \log \sigma_s (\psi)_{ij} + \phi_{ij} \cdot \log \sigma_t (\psi)_{ij}
\end{equation}

We sum up the log alignment probabilities corresponding to our target
alignments; this is minimized when the probability of alignment is high. We
are thus able to learn target alignments between source sentence $s$ and target
sentence $t$.

\begin{figure}
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (loss) {$\mathcal{L}$};
    \node[rectangle, above=0.5cm of loss] (nsum) {$-\sum$};
    \node[obs, above=2.5cm of nsum] (mask) {$\phi$};
    \node[obs, above=1cm of mask] (psi) {$\psi$};
    \node[latent, above=0.5cm of nsum, xshift=-0.75cm] (muxs) {$\times$};
    \node[latent, above=0.5cm of nsum, xshift=0.75cm] (muxt) {$\times$};
    \node[rectangle, above=0.5cm of muxs, xshift=-0.75cm] (logs) {$\log \sigma_s (\psi)$};

    \node[rectangle, above=0.5cm of muxt, xshift=0.75cm] (logt) {$\log \sigma_t (\psi)$};

    % Connect the nodes
    \edge {psi} {logs, logt};
    \edge {logs, mask} {muxs};
    \edge {logt, mask} {muxt};
    \edge {muxs, muxt} {nsum};
    \edge {nsum} {loss};

  \end{tikzpicture}
  \caption{Computation graph for our supervised loss function.
    The generated alignment matrix $\psi$ is compared to the ground
    truth alignment matrix $\phi$ to output a loss value $\mathcal{L}$.}
  \label{fig:supervised_loss}
\end{figure}

\subsection{Unsupervised Alignment Loss}

% refrence appendix

Our unsupervised loss function, to be minimized, is a 5-term equation.
We define:
\begin{itemize}[label={}]
  \item $a_t$ as the target prior alignment matrix normalized per column with respect to $t$\\ %per col wrt t
  \item $a_s$ as the source prior alignment matrix normalized per row with respect to $s$\\ %per row wrt s
  \item $\sigma_s$ as the softmax operator applied on the rows of a matrix\\
  \item $\sigma_t$ as the softmax operator applied to each column of a matrix
\end{itemize}

Indexing is performed under the assumption that source words $s_i$ form rows
and target words $t_j$ form columns. We use the term $ij$ as shorthand
for $s_i$ and $t_j$ indexing into our matrices.

\begin{equation}
  \centering
\begin{split}
  Lo&ss = \\
  &- \sum_j^{|t|} \log \left[
      \sum_i^{|s|} \exp \left(
        \log \sigma_s(\theta(t, s))_{ij} + \log \sigma_t(\psi)_{ij} \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_t(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_t(\psi)_{ij}}{a_t(i, j)} \right] \\
  &- \sum_i^{|s|} \log \left[ \sum_j^{|t|}
      \exp \left(
        \log \sigma_t(\theta(t, s))_{ij} + \log \sigma_s(\psi)_{ij}
      \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_s(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_s(\psi)_{ij}}{a_s(i, j)} \right] \\
  &- \log \sum_i^{|s|} \sum_j^{|t|} \left[
    \sigma_s(\psi) \circ \sigma_t(\psi) \right]_{ij} \\
\end{split}
\end{equation}

The full derivation of the loss function and its component terms can be found
in Appendix ~\ref{appendix:loss-function}.


\section{Train/Eval Implmentation}

\subsection{Vocabulary Building}

\subsection{Batching}

\subsection{Optimizer}
We use the Adam optimizer \cite{kingma2014adam}, which efficiently performs
gradient-based optimization for stochastic objective functions. Adam has proven
itself an effective optimizer, allowing for better performance regarding
problems with sparse gradients (i.e., natural language problems).

\subsection{Alignment Generations}
Our models generate alignment matrices $\psi$. However, these matrices are just
weights, and we need to convert them into meaningful alignments. We describe
several methods to convert our weights into actual alignments.

\subsubsection{Argmax Alignments}
The naive, simplest version of generating alignments is to align a source token
$s_i$ to a target token $t_j$ such that the alignment weight $\psi$ is
maximized. We can do this with respect to the source (row) or target (column).

\begin{equation}
  r_{ij} = \begin{cases}
    1 & \text{if } j=\argmax_k \psi_{ik} \\
    0 & \text{else}
  \end{cases}
\end{equation}

\begin{equation}
  c_{ij} = \begin{cases}
    1 & \text{if } i=\argmax_k \psi_{kj} \\
    0 & \text{else}
  \end{cases}
\end{equation}

However, this forces a single alignment for a row or column.

\subsubsection{Union, Intersection, and Grow-Diag-Final}

An improvement to the naive argmax approach is to consider the union and
intersection of the alignments generated. The union would be the element-wise
Hadamard product of the row and column alignments.

\begin{equation}
  u_{ij} = r_{ij} \cdot c_{ij}
\end{equation}

This method generates alignments that both directions agree on. However, this
is a more conservative alignement method than either argmax method alone.
Hence, we could take the intersection. This generates alignments where only one
of the original directions must align.

\begin{equation}
  n_{ij} = r_{ij} \text{ or } c_{ij}
\end{equation}

% Add grow diag final here.

\subsubsection{Thresholding}

The best approach would be to allow for alignments to be generated under a
threshold scheme. Hence we can generate multiple alignments per row (or column)
that are likely. We can then apply the Grow-Diag-Final method to prune
alignments to the union plus adjacent intersections.

We first standardize our alignment weights to have a mean of $0$ and standard
deviation of $1$. This bounds the thresholding criteria, as weights can get
arbitrarily large.

For a source-based (i.e., row-based) standardization:

\begin{equation}
  \mu_{i} = \frac{1}{|t|} \sum_j \psi_{ij}
\end{equation}
\begin{equation}
  \sigma_{i} = \sqrt{\frac{\sum_j \left( \psi_{ij} - \mu_i \right)^2}{|t|-1}}
\end{equation}
\begin{equation}
  Z_{ij} = \frac{\psi_{ij} - \mu_i}{\sigma_i}
\end{equation}

For a target-based (i.e., column-based) standardization, we compute means and
standard deviations along the columns. We can then apply a threshold to the
values to generate alignments.
% find adaptive threshold methods
\begin{equation}
  a_{ij} = \begin{cases}
    1 & \text{if } Z_{ij} > \tau \\
    0 & \text{else}
  \end{cases}
\end{equation}

The form for the thresholds $\tau$ is still yet to be determined. It will most
likely be derived from the $\mu$ and $\sigma$ to create a threshold $\tau$ that
will allow some but not all maximum values to be considered alignments. We will
investigate by graphing the histograms of true alignment $z$-scores and non
$z$-scores and find a good partition.

\section{Ground Truth Alignment Results}
% ?

\section{Data}

% talk about subtitle extraction. Also labeling of data for supervised? Or
% should that go in supervised training section. Get Bailey to mention Nikita
% metric for baseline on unaligned corpus.


\subsection{Introduction}

The efficacy of a neural network alignment system depends heavily on the
quantity and quality of the training data available. We look to leverage
existing corpora of natural language in various settings instead of the
traditional corpora (such as Europal, which is domain-specific and lacks the
unstructured and informal nature of everyday language). Training on these
commonly-used corpora leads to models with preferences for emitting language
of a parliamentary nature; we elect to focus on corpora that
provide more of a contextual breadth.

To this end, we amassed a corpus of domestic and international movie and TV
show subtitles from a variety of sources. Subtitles have the unique advantage
of being already sentence-aligned, thanks to timestamps. This alignment
somewhat holds even if timestamps are skewed between different media
formats of the same source with slightly different timings.

Our preference was to collect only professional or official subtitles, but
unfortunately these fall under copyright law and are often not publicly
available. However, there are communities of volunteer transcribers and
translators from which we can obtain unofficial subtitles. Many of these
communities have databases with publicly available dumps of all translations,
offer an API, or have a website that is scrapeable. We have employed these
tactics to obtain an initial corpus (see
\hyperref[subsec:procuring-subtitles]{Procuring Subtitles}).

We plan to handle subtitles in strict alignment (that is, those whose timestamps
correspond exactly) and will also attempt to sentence-align those in
fuzzy alignment (i.e., timestamps do not correspond one-to-one). For more on our
methodology, see \hyperref[subsec:subtitle-alignment]{Subtitle Alignment}.

For evaluating the effectiveness of our model and to facilitate
supervised training, we require ground truth alignments for some subset of our corpus. As our corpus currently has 90 million English-French aligned subtitles
(each approximately consisting of one sentence), it impractical to align by hand,
even with significant resources.
We are thus presented with an evaluation problem similar to that in Liu et al.
\cite{liu2015streaming} (though instead of the ground truth being intractable,
our ground truth is impractical to compute).

We elect to obtain ground truth alignments by a consensus-based
approach similar to that in \cite{liu2015streaming}. Broadly, we look to
well-known open-source alignment software \textit{en masse} to provide an approximation of the ground truth alignments.
We do so by evaluating the pairwise overlap between
each aligner and our model to determine a metric that can approximate
Alignment Error Rate (AER) (see
\hyperref[subsec:ground-truth-alignments]{Ground Truth Alignments}).

\subsection{Procuring Subtitles}
\label{subsec:procuring-subtitles}

Subtitles were procured via a variety of means, including downloading data
dumps; using APIs; and web scraping.

Websites like \href{https://www.opensubtitles.org/}{OpenSubtitles} offer
open source tarballs of their available subtitles. We obtained
a recent version of this dump and have begun tooling to massage it into a
uniform format so that we can import it into our corpus.

Communities like the aforementioned, while operating in a legal gray area, tend
to be on the up-and-up in that they respond to DMCA takedown requests. Other
websites adhere to these with less urgency, so we elect not to link them.
However, these sites are still useful communities of tens of thousands of
transcribers and translators whose subtitles can be useful for our corpus.

Due to their underground nature, many of these sites place strict download
limits on subtitles. To combat this, we engineered tools to cleverly extract metadata (by utilizing a breadth of different-DC hosts doing intelligent
scraping); we then make intelligent decisions on which subtitles to procure,
based on quality metrics computed from this metadata. These metrics include user subtitle rating (if available); number of downloads (normalized per
movie/episode); and number of editors/edits. The metrics can also be normalized across language, as certain languages have more subtitles than others.

Other websites avoid the legal gray area of hosting subtitles and translations
by employing deniability tactics, such as requiring the SHA256 hash of a video
file (instead of a movie title or show name) to locate subtitles. Such sites
include \href{http://thesubdb.com/}{SubDB}, which provides an open API for
accessing subtitles. We are currently exploring methods for collecting lists of
hashes of popular media (by scraping websites which list this media) that we
can feed through this website. This would have the twofold advantage of
i) allowing us to index their database for subtitles in which we are interested
and ii) giving us subtitle alignments to other sources. We also considered
reaching out to the owners of such databases, but since they only maintain
hashes, we would still need a way of associating subtitles with their source
media.


\subsubsection{Unified Data Format}

As a secondary step to procurement, we need to unify the disparate data
collected from various sources into a format that allows us to make decisions
about language pairs and easily extract pairable subtitle files for alignment.
For this purpose, we wrote custom scripting utilities for i) parsing
the data formats obtained from each of the source websites and ii) converting
all relevant data into a unified, compact binary format that is designed to be
streamed. This format allows for online streaming creation from a live stream
of incoming data and also permits out-of-memory statistical analysis and
subtitle file alignment.


\subsection{Subtitle Alignment}
\label{subsec:subtitle-alignment}

We have two goals in data processing. First, we must align
subtitle files from matching shows and episodes within sources and between
sources. Then, once we have pairs of subtitle files (each of the same medium,
but translated into different languages), we can begin the next step of
tokenizing and aligning their sentences to produce appropriate input for
our model. We can also use these pairs to derive
\hyperref[subsec:ground-truth-alignments]{Ground Truth Alignments} for the
purposes of model evaluation and supervised training.

We have several cases of subtitle file alignment to consider. Suppose we have
Subtitle file \textit{A} (in language \textit{S}) and Subtitle file \textit{B}
(in language \textit{T}). Then we have

\begin{enumerate}

    \item Files \textit{A} and \textit{B} are from the same source. Further,
        the files were grouped together on their source website under the same
        translation version. This means that one is a derivative/translation
        of the other. This can be confirmed by examining the number of lines
        in each file (and the timestamps of the subtitles). Typically these
        files are in one-to-one or near-one-to-one correspondence. We call
        these files ``exact pairs.'' Producing aligned sentences from these is
        trivial. Simply traverse the two in lock step and each each sentence
        pair you encounter.

    \item Files \textit{A} and \textit{B} are from the same source. However,
        while they are for the same medium, they aren't grouped under the
        same subtitle version. This means they may not be derivatives of
        one another (for example, they may have been independently transcribed
        and translated solely by watching the source material instead of
        consulting another subtitle). We call \textit{A} and \textit{B} a
        ``fuzzy pair.'' This case is slightly more complex to handle, because
        \textit{A} and \textit{B} may have been transcribed from different
        media formats with different timing delays. However, sentence
        alignments can still be produced by taking advantage of the fact that
        their sequential nature is still preserved. By employing some
        heuristics and interpolation, we can generate high-certainty sentence
        pairings. We plan to evaluate these pairings by analyzing in isolation
        the AER on this dataset when fed through the software we plan to use
        for \hyperref[subsec:ground-truth-alignments]{Ground Truth Alignments}.

    \item Files \textit{A} and \textit{B} are from different sources. This can
        largely be treated the same as the case above, however we first must
        determine if \textit{A} and \textit{B} are subtitles for the same media
        in different languages. We have metadata identifying languages
        \textit{S} and \textit{T} and the files, so eliminating same-language
        pairs is trivial. To match media, we employ NLP techniques to perform
        fuzzy matching on file metadata in an attempt to correlate movie/show
        title, episode/season name/number, etc. We may also consult an external
        metadata database to unify our subtitle metadata.

\end{enumerate}

Considerations must also be made to perform the aforementioned processes
efficiently, due to large corpus size.

Another consideration we plan to make is ensuring that from the breadth of
sources we don't produce duplicate subtitle pairings (if, for example, two
sources provided the exact same subtitles). This will likely consist an
extra post processing step before the data is given to the alignment model for
training.


\subsubsection{Preliminary Corpus Statistics}

We briefly summarize statistics on a subset of our corpus, which has already
been collected and processed by the data unification pipeline. A
significant portion of our corpus has been withheld from these numbers as it
is still processing. Additionally, we plan to collect data from even more
sources to negate any effects of average quality subtitles.

Looking at language prevalence, we have 84,000 English subtitle files and
23,000 French subtitle file (Figure \ref{fig:language-prevalence}). We note
that each file contains many sentences.

\begin{figure}[ht]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \multicolumn{1}{|c|}{\textbf{Language}} &
            \multicolumn{1}{|c|}{\textbf{Subtitle Files}} \\
        \hline
        English    & 84,959 \\  \hline
        French     & 23,195 \\  \hline
        Portuguese & 16,912 \\  \hline
        Italian    & 15,998 \\  \hline
        Dutch      & 10,690 \\  \hline
        Spanish    & 10,515 \\  \hline
        German     & 5,462  \\  \hline
        Romanian   & 4,726  \\  \hline
        Bulgarian  & 3,136  \\  \hline
        Greek      & 2,246  \\  \hline
    \end{tabular}

    \caption{Number of subtitle files for the top 10 most prevalent langauges
             in our corpus subset}
    \label{fig:language-prevalence}
\end{figure}

Next, we note the approximate number of sentence pairs per ``exact pair" of
subtitle languages (Figure \ref{fig:exact-pair-sentences}). There
are a significant number (5.7 million) of already aligned sentences for
English, French. However, we'd like more data than this, so we also consider
``fuzzy pairs" (Figure \ref{fig:fuzzy-pair-sentences}).

When considering ``fuzzy pairs" in addition to the ``exact pairs" we see that
we now have a plethora of sentence pairs. Notably, we have over 80 million
English-French sentence pairs and 67 million English-Italian sentence pairs.
We also have mobility between language pairs that don't include English
(55.6 million French-Italian sentence pairs, for example). Such a large corpus
of pairs allows us to comfortably train and evaluate the alignment model on
various language pairs to determine its efficacy.

\begin{figure}[ht]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \multicolumn{1}{|c|}{\textbf{Exact Language Pair}} &
            \multicolumn{1}{|c|}{\textbf{Aligned Sentences}} \\ \hline
        English, French       & 5,747,845 \\ \hline
        English, Bulgarian    & 1,918,850 \\ \hline
        French, Bulgarian     & 1,115,980 \\ \hline
        English, Portuguese   & 754,248   \\ \hline
        French, Portuguese    & 404,377   \\ \hline
        English, Spanish      & 340,123   \\ \hline
        Portuguese, Bulgarian & 211,751   \\ \hline
        English, Romanian     & 160,416   \\ \hline
        English, Greek        & 120,561   \\ \hline
        French, Romanian      & 87,082    \\ \hline
    \end{tabular}

    \caption{Top 10 approximate number of sentence pairs per ``exact pair" of
             subtitle languages}
    \label{fig:exact-pair-sentences}
\end{figure}

\begin{figure}[ht]
    \centering
    \begin{tabular}{ |l|r| }
        \hline
        \multicolumn{1}{|c|}{\textbf{Fuzzy Language Pair}} &
            \multicolumn{1}{|c|}{\textbf{Aligned Sentences}} \\
        \hline
        English, French     & 81,627,311 \\ \hline
        English, Italian    & 67,458,538 \\ \hline
        Italian, French     & 55,623,242 \\ \hline
        English, Portuguese & 55,487,067 \\ \hline
        French, Portuguese  & 43,822,977 \\ \hline
        English, Dutch      & 41,648,439 \\ \hline
        English, Spanish    & 39,650,085 \\ \hline
        Italian, Portuguese & 36,098,391 \\ \hline
        French, Dutch       & 35,809,672 \\ \hline
        English, German     & 32,495,335 \\ \hline
        Spanish, French     & 31,086,831 \\ \hline
        Italian, Dutch      & 29,880,240 \\ \hline
        French, German      & 27,358,847 \\ \hline
        Portuguese, German  & 26,362,749 \\ \hline
        Spanish, Italian    & 26,182,558 \\ \hline
        Italian, German     & 23,596,809 \\ \hline
        Spanish, Portuguese & 23,451,001 \\ \hline
        English, Romanian   & 22,800,267 \\ \hline
        Portuguese, Dutch   & 21,380,712 \\ \hline
        English, Bulgarian  & 20,239,688 \\ \hline
    \end{tabular}

    \caption{Top 20 number of approximate sentence pairs per ``fuzzy pair" of
             subtitle languages}
    \label{fig:fuzzy-pair-sentences}
\end{figure}

We make one final important note that these estimations are based off of
subtitle file metadata. The exact number of sentence pairs won't be known until
the subtitle file alignment step is completed. This approximation assumes that
each subtitle line corresponds to one sentence. We believe this to be a
reasonable assumption, but it may be the case that in practice that this is
correspondence is smaller.


\subsection{Ground Truth Alignments}
\label{subsec:ground-truth-alignments}

For evaluating the Alignment Error Rate (AER) of our alignment model and to
offer the possibility of supervised training, we require ground truth
alignments. With smaller corpora like
\href{https://www.isi.edu/natural-language/download/hansard/}{\textit{Handsards}},
human alignments are tractable given the relatively small corpus size
(approximately 1 million sentence pairs). However, for our significantly larger
corpus, this poses a problem. Its size and our limited resources preclude us
from obtaining ground truth human alignments.

To combat this, we employ an ensemble approach similar to that in Liu et al.
\cite{liu2015streaming}. In particular, we plan to align our dataset with
the
\href{http://nlp.cs.berkeley.edu/projects/historical.shtml#WordAligner}{Berkley Aligner},
\href{https://clear.colorado.edu/CompSemWiki/index.php/MGIZA\%2B\%2B}{MGIZA},
\href{https://github.com/clab/fast\_align}{Fast Align}, and
\href{https://anymalign.limsi.fr/}{Anymalign} first. We will then compare the
pairwise results of the aligners. By evaluating their AERs and
overlaps/differences, we can derive an approximate gold standard alignment. For
example, all of the aligners agreeing on a certain word pair should give good
credence to it being a ground truth pair. We plan to weight their contributions
by the inverse of their pairwise relative AERs. From this, we can easily derive
ground truth alignments.

For evaluating the performance of our alignment model, we will consider its
AER against our hypothesized ground truth, as well as its pairwise AER against
each ground truth aligner (weighted by the inverse of that aligner's pairwise
relative AERs within our ground truth alignment group, as before).

\section{Future Work}
% what to get done by final report

%------------------------------------------------------------------------------

% References
\bibliographystyle{abbrv}
\bibliography{interim_report}

%------------------------------------------------------------------------------

\clearpage
\appendix
\onecolumn
\begin{myparindent}{0pt}
\section{Unsupervised Loss Function Derivation}
\label{appendix:loss-function}
To create our loss function, we convert our alignment matrix $\psi$ to a
probability distribution. We define $\sigma(\vec{x})$ as the softmax operator
applied on vector $\vec{x}$, and $\sigma(\vec{x})_i$ as the $x_i$ softmax
probability for vector $\vec{x}$.

\begin{equation}
\sigma(\vec{x})_i = \frac{\exp(x_i)}{\sum_j\exp(x_j)}
\end{equation}

We therefore define two operations on the alignment matrix $\psi$. For source
$s$ to target $t$ probability generations, we define $\sigma_t(\psi)$ as the
softmax on each column, i.e., target word $t_j$.
\begin{equation}
  \sigma_t(\psi) = \left[
    \begin{matrix}
      \sigma(\psi_{t_1}) &
      \hdots &
      \sigma(\psi_{t_j}) &
      \hdots &
      \sigma(\psi_{t_m})  \\
    \end{matrix}
\right]
\end{equation}
Equivalently,

\begin{equation}
  \sigma_t(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{kj}}}
\end{equation}

In addition, we further define the target-to-source generation as
$\sigma_s(\psi)$, where the softmax operator is applied on each row for target
word $s_i$ and produces a row vector.
\begin{equation}
  \sigma_s(\psi) = \left[
    \begin{matrix}
      \sigma(\psi_{s_1})  \\
      \vdots \\
      \sigma(\psi_{s_i})  \\
      \vdots \\
      \sigma(\psi_{s_n})  \\
    \end{matrix}
\right]
\end{equation}
Equivalently,

\begin{equation}
  \sigma_s(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{ik}}}
\end{equation}

% todo: viv
While variational autoencoders (VAEs) sample from the probability distribution
derived by the neural network, our model's distribution is discrete.
Therefore, we need not sample and can instead compute the loss in terms of all
possibilities.

\subsection{Probability Maximization}

In our original formulation, we maximized probabilities for word alignments.
For this model, we define $p(t, s | \theta, \psi)$ as such:

\begin{equation}
  p(t, s | \theta, \psi)
    = \prod_j^{|t|} \sum_i^{|s|} p_\theta(t_j| s_i) \cdot p_a(i|j)
    = \prod_j^{|t|} \sum_i^{|s|} \sigma_s(\theta(t_j, s_i)) \cdot \sigma_t(\psi)_{ij}
\end{equation}

This formula stems from our assumption that the alignments for each target word
are conditionally independent, hence a product over target words $t_j$. We then
sum over all the alignment possibilities from the source words $s_i$, thus
marginalizing over the alignments.

For $s \mapsto t$ alignments, alignments are softmaxed per column because we
hold the target $t_j$ constant and iterate over the source $s_i$, hence
$\sigma_t$.

The translation probabilities are softmaxed per row (i.e., source word $s_i$)
since we are given a source word and want to know the probability of
translating $s_i$ to $t_j$, hence $\sigma_s$.

However, as we do not maximize in neural networks, we must transform this
equation into a minimization problem by taking the negative log of $p$:

\begin{equation}
  -\log p(t, s | \theta, \psi) =
  - \sum_j^{|t|}
     \log \left[ \sum_i^{|s|} \sigma_s \left( \theta(t_j, s_i) \right) \cdot
      \sigma_t(\psi)_{ij} \right]
\end{equation}

We can simplify this into the equation below, for easier implementation
(via PyTorch's \texttt{logsumexp} function):

\begin{equation}
  -\log  p(t , s | \theta, \psi) =
  - \sum_j^{|t|}  \log \left[ \sum_i^{|s|} \exp
      \left( \log \sigma_s(\theta(t_j, s_i)) + \log \sigma_t(\psi)_{ij} \right)
    \right]
\end{equation}

This is the term to be minimized for our source-to-target word alignment
generation.


\subsection{KL Divergence and Prior Terms}

We also need to minimize the Kullback-Leibler (KL) divergence between our
distribution and a prior. For discrete probability distributions $P$ and $Q$
defined on the same probability space, the reverse KL divergence from $Q$ to
$P$ is defined as:

\begin{equation}
D_{\mathrm{KL}}(Q \| P) = \sum_{i} Q(i) \log \left( \frac{Q(i)}{P(i)} \right)
\end{equation}

In other words, it is the expectation of the log difference between the
probabilities $P$ and $Q$, where the expectation is taken using the
probabilities $Q$.

To calculate the KL divergence of our model, we must define distributions $P$
and $Q$. We first consider $P$ as our prior distribution, henceforth called
$a_t$. This prior distribution $a_t$ is a $n \times m$ matrix filled with the
alignment probabilities filled for source word $s_i$; target word $t_j$; source
sentence $s$ of length $n$; and target sentence $t$ of length $m$:

We define distortion exponent $h$ as:
\begin{equation}
  h(i, j) = {-\lambda \left| \frac{i}{n} - \frac{j}{m}\right|}
\end{equation}

% might have to flip this to be sum over i' not j' DONE
\begin{equation}
  Z_j = \sum_{i'} \exp h(i', j)
\end{equation}

\begin{equation}
a_t (i, j) =
\begin{cases}
      p_0 & \text{if } null \\
     (1-p_0) \cdot \frac{e^{h(i,j)}}{Z_j} & \text{else}
   \end{cases}
\end{equation}

In \cite{dyer2013simple}, parameter values were selected as $\lambda = 4$ and
$p_0 = 0.08$ for the entire corpus. Each element value of $a_t$ is normalized
by the sum of the column distortion values to create a valid distribution via
term $Z_j$ (since we hold target $t_j$ constant). For future notation, let the
subscript on the distribution $a$ denote the way we normalize, i.e., $a_t$ is
the prior alignment distribution normalized with respect to target words $t_j$
and thus normalized by the sum of the column for $t_j$.

% \todo: change?
We can then write our KL Divergence (to be minimized) as:

\begin{equation}
  D_{\mathrm{KL}}(\sigma_t(\psi) \| a_t) =
    \sum_i^n \sum_j^m \sigma_t(\psi)_{ij} \cdot
      \log \left[ \frac{\sigma_t(\psi)_{ij}}{a_t(i, j)} \right]
\end{equation}


\subsection{Target-to-Source Loss Terms}

We have described the loss terms for a source $s$ to target $t$ word alignment.
However, we seek to perform alignment by agreement. Hence, we must add loss
functions that describe the evaluation of target $t$ to source $s$:

\begin{equation}
  -\log  p(t , s | \theta, \psi) =
  - \sum_i^{|s|}  \log \left[ \sum_j^{|t|}
      \exp \left(
        \log \sigma_t(\theta(t_j, s_i)) + \log \sigma_s(\psi)_{ij}
      \right)
    \right]
\end{equation}

\noindent
Additionally, for the KL Divergence term for target $t$ to source $s$:

\begin{equation}
D_{\mathrm{KL}} (\sigma_s(\psi) \| a_s) = \sum_i^n \sum_j^m \sigma_s(\psi)_{ij}
  \cdot \log \left[ \frac{\sigma_s(\psi)_{ij}}{a_s(i, j)} \right]
\end{equation}

\noindent
where the normalization of the prior $a_s$ is on the row instead of the column.
In other words, we hold the source $s_i$ constant; sum the row; and divide each
row element by the sum.

The equations for $a_s$ are as follows:

\begin{equation}
  h(i, j) = {-\lambda \left| \frac{i}{n} - \frac{j}{m}\right|}
\end{equation}

% might have to flip this to be sum over i' not j' DONE
\begin{equation}
  Z_i = \sum_{j'} \exp h(i, j')
\end{equation}

\begin{equation}
a_s (i, j) =
\begin{cases}
      p_0 & \text{if } null \\
     (1-p_0) \cdot \frac{e^{h(i,j)}}{Z_i} & \text{else}
   \end{cases}
\end{equation}

We also perform the softmax operator on each column of the $\psi$ matrix,
previously defined as $\sigma_t(\psi)$.


\subsection{Alignment by Agreement}

Finally, one last loss function term must be added to jointly train each model.
We define $\circ$ as the Hadamard Product, which is the element-wise
multiplication of two matrices. For instance:
$(A \circ B)_{ij} = A_{ij} \cdot B_{ij}$.

We can write the term as such:

\begin{equation}
  -\log \sum_i^{|s|} \sum_j^{|t|}
    \left[ \sigma_s(\psi) \circ \sigma_t(\psi) \right]_{ij}
\end{equation}

% Again note here, you can use logsumexp trick to implement this efficiently in
% PyTorch


\subsection{Combined Loss Function}

Our final loss function, to be minimized, is a 5-term equation. We define:
\begin{itemize}[label={}]
  \item $a_t$ as the target prior alignment matrix normalized per column with respect to $t$\\ %per col wrt t
  \item $a_s$ as the source prior alignment matrix normalized per row with respect to $s$\\ %per row wrt s
  \item $\sigma_s$ as the softmax operator applied on the rows of a matrix\\
  \item $\sigma_t$ as the softmax operator applied to each column of a matrix
\end{itemize}

Indexing is done under the assumption that source words $s_i$ form rows
and target words $t_j$ form columns. The term $ij$ is a shorthand
for $s_i$ and $t_j$ indexing into our matrices.

\begin{equation}
  \centering
\begin{split}
  Lo&ss = \\
  &- \sum_j^{|t|} \log \left[
      \sum_i^{|s|} \exp \left(
        \log \sigma_s(\theta(t, s))_{ij} + \log \sigma_t(\psi)_{ij} \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_t(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_t(\psi)_{ij}}{a_t(i, j)} \right] \\
  &- \sum_i^{|s|} \log \left[ \sum_j^{|t|}
      \exp \left(
        \log \sigma_t(\theta(t, s))_{ij} + \log \sigma_s(\psi)_{ij}
      \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_s(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_s(\psi)_{ij}}{a_s(i, j)} \right] \\
  &- \log \sum_i^{|s|} \sum_j^{|t|} \left[
    \sigma_s(\psi) \circ \sigma_t(\psi) \right]_{ij} \\
\end{split}
\end{equation}

% Note, the alignment prior is not exactly the same for source-to-target and
% target-to-source because even though the loss function is symmetric, the
% probabilities are normalized with respect to n or m, depending which
% direction you are translating
\end{myparindent}
\end{document}

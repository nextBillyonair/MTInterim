%------------------------------------------------------------------------------
%   PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

\usepackage[english]{babel} % Language hyphenation and typographical rules

% Document margins
\usepackage[hmarginratio=1:1,top=32mm,left=20mm,right=20mm,columnsep=20pt]{geometry}
% Custom captions under/above floats in tables or figures
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption}
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact
\usepackage{textcomp}

% Allows abstract customization
\usepackage{abstract}
% Set the "Abstract" text to bold
\renewcommand{\abstractnamefont}{\normalfont\bfseries}
% Set the abstract itself to small italic text
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{\thetitle}
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{amsmath}

\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{color}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{graphicx}

\captionsetup[figure]{labelfont={bf},textfont=normalfont}

%------------------------------------------------------------------------------
%   TITLE SECTION
%------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting

\title{MT Interim Report: \\ Neural Word Alignment}
\author{%
\textsc{Bailey Parker} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:bailey@jhu.edu}{bailey@jhu.edu}
 \and
 \textsc{Vivian Tsai} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:viv@jhu.edu}{viv@jhu.edu}
 \and
  \textsc{William Watson} \\[1ex]
\normalsize Johns Hopkins University \\
\normalsize \href{mailto:billwatson@jhu.edu}{billwatson@jhu.edu}
}

\date{}

%------------------------------------------------------------------------------
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\qdist}[1]{\ifmmode\langle#1\rangle\else\textlangle#1\textrangle\fi}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newlength\mystoreparindent
\newenvironment{myparindent}[1]{%
  \setlength{\mystoreparindent}{\the\parindent}
  \setlength{\parindent}{#1}
  }{%
  \setlength{\parindent}{\mystoreparindent}
}

\begin{document}

% Print the title
\maketitle

% \section{Introduction}

% TODO

%------------------------------------------------------------------------------

\begin{abstract}
% \noindent \blindtext
We discuss our intial progress on our Neural Word Alignment model. More
specifically, we detail our data procurement methods and processing algorithms;
elaborate on our current experimental models (including the Dot Aligner and
Bidirectional GRU Aligner); and define our loss function, train/evaluation
implementation, and batching techniques.
\end{abstract}

%------------------------------------------------------------------------------
\section{Introduction}
% @viv
% \begin{itemize}
%   \item Data procurement
%   \item Baseline algorithms
%   \item Preliminary experiments % (i.e., determining)
%   \item Outlining planned work
% \end{itemize}

%\section{Data Procurement and Processing}
%\subsection{Symbol Tokenization}
%\subsection{Number Tokenization}
%\subsection{Proper Noun Tokenization}
%\subsection{Lemmatization Techniques}
%\subsection{POS Tagging}

% \section{Model Components}
\section{Background}
% inputs, lambdas, scaling facotrs, etc, anything on notation
We begin by defining several operations frequently used in our experiments
and discussion.

\subsection{Preliminary Notation}

\subsubsection{Hadamard Product}

To perform element-wise multiplication of two matrices $A$ and $B$ with
equivalent dimensions, we use the Hadamard product, defined as the $\circ$:
\begin{equation}
  (A \circ B)_{ij} = A_{ij} \cdot B_{ij}
\end{equation}

\subsubsection{Sigmoid Function}

The sigmoid function $\sigma$ is applied element-wise and defined as follows:
\begin{equation}
  \sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}

\subsubsection{Hyperbolic Tangent Function}

The hyperbolic tangent function $\tanh$ is defined as follows:
\begin{equation}
  \tanh(x) = \frac{e^x - e^{-x}} {e^x + e^{-x}}
\end{equation}
and is applied element-wise.

\subsubsection{Variables}
We define our source sequence as $s$ and target sequence as $t$ (and
consequently, the $i$-th source token as $s_i$ and the $j$-th target token as
$t_j$).
We also refer to a matrix $\psi$ whose rows represent values related to source
tokens $s_i$ and whose columns refer to values related to target tokens $t_j$.
Hence, $\psi$ is of size $|s| \times |t|$.

\subsection{Softmax and Log Softmax}
The softmax function transforms a vector of values into a probability
distribution. Applying the softmax function to an $n$-dimensional input tensor
rescales it so that the elements of the $n$-dimensional output tensor lie in
range $(0,1)$ and sum to $1$.
\begin{equation}
  \sigma(\vec{x})_i = \frac{e^{x_i}}{\sum_j e^{x_j}}
\end{equation}

For matrix $\psi$, rows correspond to source words $s_i$ and columns correspond
to target words $t_j$. When we softmax with respect to the target words, i.e.,
the softmax is applied per column and denoted as $\sigma_t(\psi)$
(Equation \ref{eq:softmax-target}); when we softmax with respect to the source
words, the softmax is applied per row and denoted as $\sigma_s(\psi)$
(Equation \ref{eq:softmax-source}).
\begin{equation}
  \label{eq:softmax-target}
  \sigma_t(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{kj}}}
\end{equation}
\begin{equation}
  \label{eq:softmax-source}
  \sigma_s(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{ik}}}
\end{equation}
% reword
While calculating loss, we work in log space and accordingly use the log
softmax operator for numerical stability.
\begin{equation}
  \log \sigma_t(\psi)_{ij} = \log \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{kj}}}
\end{equation}
\begin{equation}
  \log \sigma_s(\psi)_{ij} = \log \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{ik}}}
\end{equation}

\subsection{Word Embeddings}
Word embedding layers allow for a simple lookup table that stores embeddings
of a fixed dictionary and size. More specifically, these layers are often used
to store word embeddings and retrieve them using indices. The input to the
embedding layer is a list of indices, and the output is the corresponding
word embeddings. This allows words to be represented numerically to a set
embedding dimension size, and thus can be passed onto later layers.

Word embeddings can be formulated as a weight matrix $W_e$, where the
vector representation of word $w_i$ is the $i$-th row of the matrix, and
can be represented as $W_e[w_i]$.

\begin{equation}
  W_e = \begin{bmatrix}
  \longleftarrow w_1 \longrightarrow \\
  \vdots\\
  \longleftarrow w_i \longrightarrow\\
  \vdots\\
  \longleftarrow w_n \longrightarrow
\end{bmatrix}
\end{equation}

\subsection{Gated Recurrent Units (GRU)}
\label{sec:gru}

Gated Recurrent Units (GRUs) are a more complex formulation to recurrent layers
to process sequences, and improve the vanishing gradient problem found in
vanilla RNN layers while using less parameters than a Long Short-Term Memory
(LSTM) layer \cite{cho2014learning}.

A GRU makes use of three gates: the $r_t$  reset gate; the $z_t$ update gate;
and the $n_t$ new gate. These allow for a blending of the hidden state with new input. The gates are computed as follows for each input $x_t$ in a sequence
$\vec{x}$:
\begin{equation}
  \begin{split}\begin{array}{ll}
  r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{t-1} + b_{hr}) \\
  z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{t-1} + b_{hz}) \\
  n_t = \tanh(W_{in} x_t + b_{in} + r_t \circ (W_{hn} h_{t-1}+ b_{hn})) \\
  h_t = (1 - z_t) \circ n_t + z_t \circ h_{t-1} \\
  \end{array}\end{split}
\end{equation}
where $x_t$ is input at time $t$, $h_{t-1}$ is the hidden state of the
previous layer at time $t-1$ or the initial hidden state at time $0$, $h_t$
is the new hidden state at time $t$, and $\sigma$ is the sigmoid function.

\subsection{Bidirectional GRU}
\label{sec:bidirectional}

GRUs only process sequences in a forward direction. However, for
translation and in general NLP, we also care about the succeeding input.
%@viv rephrase
We thus introduce the Bidirectional GRU, which that runs two separate GRU
layers in opposite directions and stacks the output such that each element has
the context of elements before and after it.

Let us define $\operatorname{GRU}(input)$ as a GRU layer that outputs the
hidden cells from $input$ on a single forward pass. Let us also define the
operator $\overrightarrow{h}$ as the tensor with elements ordered in
order $[0,$ $n]$, and $\overleftarrow{h}$ as the tensor elements ordered
from $[n,$ $0]$, i.e. reversed.

When flipping the arrow, i.e., from $\overrightarrow{h}$ to
$\overleftarrow{h}$, we define this operation as an invert/reverse
function, and it flips the elements to the opposite orientation. We
additionally declare $\Vert$ as the concatenation operator. Finally, we
define $\overrightarrow{h_f}$ as the forward output, $\overleftarrow{h_b}$
as the backward output, and $h_o$ as the final, stacked bidirectional output.

\begin{equation}
  \label{eq:bidirectional}
  \begin{split}
    \begin{array}{ll}
      % fix to have forward and reverse arrows, and update top
      \overrightarrow{h_f} = \operatorname{GRU}(\overrightarrow{input})\\
      \\
      \overleftarrow{h_b} = \operatorname{GRU}(\overleftarrow{input})\\
      \\
      h_o = \overrightarrow{h_f} \,\,\Vert \,\, \overrightarrow{h_b}\\
      \\
    \end{array}
  \end{split}
\end{equation}

From the above equations, we compute the forward output normally; compute the
backwards output on the inverted input; and concatenate the forward outputs
with the inverted backwards outputs. All final hidden states are provided as
well. The output is of size $(N,$ $batch,$ $hidden$ $size*2)$ since we
concatenate the outputs of the two GRU layers.

\subsection{Alignment Prior}
\label{sec:alignment_prior}
In order to learn diagonal alignments, we describe our formulation of the
IBM Model 2 reparameterization as a custom PyTorch layer. We define the prior
alignment matrix $A$ as an $|s| \times |t|$. Given a source token $s_i$ and
target token $t_j$, and alignment hyperparameter $\lambda$, we define the
alignment distortion function $a_{ij}$:
\begin{equation}
  a_{ij} = -\lambda | i - j |
\end{equation}

We set the default $\lambda$ value to $4$. This encodes our diagonal prior,
and allows us to add in the diagonal weights to our network's weights. We
allow for an optional, learnable scaling factor $\alpha$ to control the
strength of this prior, defaulted to $0.25$. Hence our IBM Model 2 module
outputs the alignemnt prior matrix $A$:
\begin{equation}
  A_{ij} = \alpha \cdot a_{ij}
\end{equation}

\subsection{Batch Matrix Multiplication\\(BMM)}
\label{sec:bmm}
To combine source and target tensors, we use batch matrix multiplication (BMM).
Given a matrix $A$ of size $(b,$ $n,$ $m)$ and matrix $B$ of size
$(b,$ $m,$ $p)$, we perform matrix multiplication on the sub-matrices to
create an output matrix $O$ of size $(b$ $n,$ $p)$.

\begin{equation}
  O_i = A_i \times B_i
\end{equation}

This allows us to combine the vectorized output of our network on each
individual sequence and combine them into a matrix tensor of size
$(b,$ $|s|,$ $|t|)$. This requires the use of simple transpose functions to
align the dimensions of each pairing sentence to correctly compute the BMM.

\section{Model Descriptions}
We use the aformentioned model components to construct complex alignment
models, and provide a more through discussion on our techniques.

\subsection{Dot Aligner}
This is our baseline model. This model attempts to learn word embeddings
to correctly give alignemnt outputs.

The intuition behind this baseline model was to compute dot products
between every source token $s_i$ embedding and target token $t_j$ embedding
to generate an alignment matrix via BMM (Section ~\ref{sec:bmm}), combined
with the diagonal prior discussed in Section ~\ref{sec:alignment_prior}.
The output is the alignment matrix $\psi$.

This is a very simple model that only learns the pure embeddings; we
therefore do not expect it to perform as well as more complex models.
The full computation graph can be seen in Figure ~\ref{fig:dot_aligner}.

\begin{figure}
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (plus) {$+$};
    \node[latent, below=1cm of plus] (psi) {$\psi$};
    \node[rectangle, above=0.75cm of plus, xshift=-2.25cm] (bmm) {BMM};
    \node[rectangle, above=0.5cm of bmm, xshift=-1cm] (embed_s) {Embed};
    \node[rectangle, above=0.5cm of bmm, xshift=1cm] (embed_t) {Embed};
    \node[obs, above=0.75cm of embed_s] (s) {$s$};
    \node[obs, above=0.75cm of embed_t] (t) {$t$};
    \node[obs, right=0.6cm of t] (lamb) {$\lambda$};
    \node[obs, right=0.6cm of lamb] (i) {$i$};
    \node[obs, right=0.6cm of i] (j) {$j$};
    \node[rectangle, below=0.75cm of i] (align) {$a_{ij} = -\lambda |i-j|$};
    % \node[rectangle, below=1cm of align] (logsoft) {$\log \sigma_t(a)$};
    \node[latent, below=0.75cm of align] (mux) {$\times$};
    \node[latent, right=0.5cm of align] (alpha) {$\alpha$};


    % Connect the nodes
    \edge {s} {embed_s};
    \edge {t} {embed_t};
    \edge {embed_s, embed_t} {bmm};
    \edge {lamb, i, j} {align};
    % \edge {align} {logsoft};
    \edge {align, alpha} {mux};
    \edge {mux, bmm} {plus};
    \edge {plus} {psi};

  \end{tikzpicture}
  \caption{Model Architecture for Dot Aligner for source $s$,
  target $t$ alignments. $\alpha$ is a global learnable scaling factor for
  the importance of alignment distribution $a$, and $\lambda$ is the
  global alignment distortion parameter.}
  \label{fig:dot_aligner}
\end{figure}

\subsection{Bidirectional GRU Aligner}

An extension to the capacity of the Dot Aligner is to add a bidirectional GRU
after the embedding layers. This will allow the model to consider not only
forward propagation of the input sequence, but also the reverse. Therefore, our
vectorized encodings of each sequence has the context of the words in its local
area for context. The downstream algorithm for forward propagation is the same
after the recurrent layers, as detailed in Figure ~\ref{fig:gru_aligner}.

\begin{figure}
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (plus) {$+$};
    \node[latent, below=1cm of plus] (psi) {$\psi$};
    \node[rectangle, above=0.5cm of plus, xshift=-2.25cm] (bmm) {BMM};
    \node[rectangle, above=0.5cm of bmm, xshift=-1cm] (gru_s) {GRU};
    \node[rectangle, above=0.5cm of bmm, xshift=1cm] (gru_t) {GRU};
    \node[rectangle, above=0.5cm of gru_s] (embed_s) {Embed};
    \node[rectangle, above=0.5cm of gru_t] (embed_t) {Embed};
    \node[obs, above=0.75cm of embed_s] (s) {$s$};
    \node[obs, above=0.75cm of embed_t] (t) {$t$};
    \node[obs, right=0.6cm of t] (lamb) {$\lambda$};
    \node[obs, right=0.6cm of lamb] (i) {$i$};
    \node[obs, right=0.6cm of i] (j) {$j$};
    \node[rectangle, below=0.75cm of i] (align) {$a_{ij} = -\lambda |i-j|$};
    % \node[rectangle, below=1cm of align] (logsoft) {$\log \sigma_t(a)$};
    \node[latent, below=1cm of align] (mux) {$\times$};
    \node[latent, right=0.5cm of align] (alpha) {$\alpha$};

    % Connect the nodes
    \edge {s} {embed_s};
    \edge {t} {embed_t};
    \edge {embed_s} {gru_s};
    \edge {embed_t} {gru_t};
    \edge {gru_s, gru_t} {bmm};
    \edge {lamb, i, j} {align};
    % \edge {align} {logsoft};
    \edge {align, alpha} {mux};
    \edge {mux, bmm} {plus};
    \edge {plus} {psi};

  \end{tikzpicture}
  \caption{Model Architecture for Bidirectional GRU Aligner for source $s$,
  target $t$ alignments. $\alpha$ is a global learnable scaling factor for the
  importance of alignment distribution $a$, and $\lambda$ is the global
  alignment distortion parameter.}
  \label{fig:gru_aligner}
\end{figure}

\subsection{Extensions}
We discuss possible extensions for the two aformentioned models intended to
improve the capacity for learning alignments (based upon our initial results).

\section{Loss Function}
% quick tldr, point to appendix for each sub thing, just describe
% it
We describe two modes of training: supervised and unsupervised. Supervised
training allows us to measure the capacity of our models to learn given
alignments and is the first step in validating our model's ability to
eventually learn unsupervised alignments.

\subsection{Supervised Loss}
We can formulate our supervised loss as maximizing the probabilities of our
alignments, $\psi$, with the ground truth alignments, represented as $\phi$.

Our targets are binary-valued alignment matrices of size $|s| \times |t|$.
Targets $\phi$ are defined as follows:

\begin{equation}
  \phi_{ij} = \begin{cases}
  1 & \text{if } s_i \text{ aligns with } t_j \\
  0 & \text{else}
  \end{cases}
\end{equation}

\noindent for source token $s_i$ and target token $t_j$.

We now formulate our loss objective as the maximum likelihood estimate (MLE)
between our alignment matrix $\psi$ and target matrix $\phi$. However, $\psi$
are hard weights, and we use the softmax function to convert our alignment
weights to valid probabilities. In addition, we seek to maximize the
probability for both the row ($\sigma_s$) and column ($\sigma_t$) softmax.

Our MLE function to be maximized is then:

\begin{equation}
  MLE(\psi, \phi) = \prod_i \prod_j \left[ \frac{\exp \psi_{ij}}{\sum_k \exp \psi_{ik}} \right]^{\phi_{ij}} \cdot \left[ \frac{\exp \psi_{ij}}{\sum_k \exp \psi_{kj}} \right]^{\phi_{ij}}
\end{equation}

\begin{equation}
  MLE(\psi, \phi) = \prod_i \prod_j \left[ \sigma_s(\psi)_{ij} \right]^{\phi_{ij}} \cdot \left[ \sigma_t(\psi)_{ij} \right]^{\phi_{ij}}
\end{equation}

We can see that maximizing the MLE function allows our network to drive the
probabilities of alignment to be as close to $1$ as possible, and implicity
depresses all other values along the row and column. However, neural networks
do not maximize objective functions, and the multiplication of small
probabilities leads to issues of numerical stability. We therefore define the
following negative log likelihood (NLL) function:

\begin{equation}
  NLL(\psi, \phi) = - \sum_i \sum_j  \phi_{ij} \cdot \log \sigma_s (\psi)_{ij} + \phi_{ij} \cdot \log \sigma_t (\psi)_{ij}
\end{equation}

We sum up the log alignment probabilities corresponding to our target
alignments; this is minimized when the probability of alignment is high. We
are thus able to learn target alignments between source sentence $s$ and target
sentence $t$.

\begin{figure}
  \centering
  \begin{tikzpicture}

    % Define nodes
    \node[latent] (loss) {$\mathcal{L}$};
    \node[rectangle, above=0.5cm of loss] (nsum) {$-\sum$};
    \node[obs, above=2.5cm of nsum] (mask) {$\phi$};
    \node[obs, above=1cm of mask] (psi) {$\psi$};
    \node[latent, above=0.5cm of nsum, xshift=-0.75cm] (muxs) {$\times$};
    \node[latent, above=0.5cm of nsum, xshift=0.75cm] (muxt) {$\times$};
    \node[rectangle, above=0.5cm of muxs, xshift=-0.75cm] (logs) {$\log \sigma_s (\psi)$};

    \node[rectangle, above=0.5cm of muxt, xshift=0.75cm] (logt) {$\log \sigma_t (\psi)$};

    % Connect the nodes
    \edge {psi} {logs, logt};
    \edge {logs, mask} {muxs};
    \edge {logt, mask} {muxt};
    \edge {muxs, muxt} {nsum};
    \edge {nsum} {loss};

  \end{tikzpicture}
  \caption{Computation graph for our supervised loss function.
    The generated alignment matrix $\psi$ is compared to the ground
    truth alignment matrix $\phi$ to output a loss value $\mathcal{L}$.}
  \label{fig:supervised_loss}
\end{figure}

\subsection{Unsupervised Alignment Loss}

% refrence appendix

Our unsupervised loss function, to be minimized, is a 5-term equation.
We define:
\begin{itemize}[label={}]
  \item $a_t$ as the target prior alignment matrix normalized per column with respect to $t$\\ %per col wrt t
  \item $a_s$ as the source prior alignment matrix normalized per row with respect to $s$\\ %per row wrt s
  \item $\sigma_s$ as the softmax operator applied on the rows of a matrix\\
  \item $\sigma_t$ as the softmax operator applied to each column of a matrix
\end{itemize}

Indexing is performed under the assumption that source words $s_i$ form rows
and target words $t_j$ form columns. We use the term $ij$ as shorthand
for $s_i$ and $t_j$ indexing into our matrices.

\begin{equation}
  \centering
\begin{split}
  Lo&ss = \\
  &- \sum_j^{|t|} \log \left[
      \sum_i^{|s|} \exp \left(
        \log \sigma_s(\theta(t, s))_{ij} + \log \sigma_t(\psi)_{ij} \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_t(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_t(\psi)_{ij}}{a_t(i, j)} \right] \\
  &- \sum_i^{|s|} \log \left[ \sum_j^{|t|}
      \exp \left(
        \log \sigma_t(\theta(t, s))_{ij} + \log \sigma_s(\psi)_{ij}
      \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_s(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_s(\psi)_{ij}}{a_s(i, j)} \right] \\
  &- \log \sum_i^{|s|} \sum_j^{|t|} \left[
    \sigma_s(\psi) \circ \sigma_t(\psi) \right]_{ij} \\
\end{split}
\end{equation}

The full derivation of the loss function and its component terms can be found
in Appendix ~\ref{appendix:loss-function}.


\section{Train/Eval Implmentation}
\subsection{Vocabulary Building}
\subsection{Batching}
\subsection{Optimizer}
% Adam
\subsection{Alignment Generations}
Our models generate alignment matrices $\psi$. However, these matrices are just
weights, and we need to convert them into meaningful alignments. We describe
several methods to convert our weights into actual alignments.

\subsubsection{Argmax Alignments}
The naive, simplest version of generating alignments is to align a source token
$s_i$ to a target token $t_j$ such that the alignment weight $\psi$ is
maximized. We can do this with respect to the source (row) or target (column).

\begin{equation}
  r_{ij} = \begin{cases}
    1 & \text{if } j=\argmax_k \psi_{ik} \\
    0 & \text{else}
  \end{cases}
\end{equation}

\begin{equation}
  c_{ij} = \begin{cases}
    1 & \text{if } i=\argmax_k \psi_{kj} \\
    0 & \text{else}
  \end{cases}
\end{equation}

However, this forces a single alignment for a row or column.

\subsubsection{Union, Intersection, and Grow-Diag-Final}

An improvement to the naive argmax approach is to consider the union and
intersection of the alignments generated. The union would be the element-wise
Hadamard product of the row and column alignments.

\begin{equation}
  u_{ij} = r_{ij} \cdot c_{ij}
\end{equation}

This method generates alignments that both directions agree on. However, this
is a more conservative alignement method than either argmax method alone.
Hence, we could take the intersection. This generates alignments where only one
of the original directions must align.

\begin{equation}
  n_{ij} = r_{ij} \text{ or } c_{ij}
\end{equation}

% Add grow diag final here.

\subsubsection{Thresholding}

The best approach would be to allow for alignments to be generated under a
threshold scheme. Hence we can generate multiple alignments per row (or column)
that are likely. We can then apply the Grow-Diag-Final method to prune
alignments to the union plus adjacent intersections.

We first standardize our alignment weights to have a mean of $0$ and standard
deviation of $1$. This bounds the thresholding criteria, as weights can get
arbitrarily large.

For a source-based (i.e., row-based) standardization:

\begin{equation}
  \mu_{i} = \frac{1}{|t|} \sum_j \psi_{ij}
\end{equation}
\begin{equation}
  \sigma_{i} = \sqrt{\frac{\sum_j \left( \psi_{ij} - \mu_i \right)^2}{|t|-1}}
\end{equation}
\begin{equation}
  Z_{ij} = \frac{\psi_{ij} - \mu_i}{\sigma_i}
\end{equation}

For a target-based (i.e., column-based) standardization, we compute means and
standard deviations along the columns. We can then apply a threshold to the
values to generate alignments.
% find adaptive threshold methods
\begin{equation}
  a_{ij} = \begin{cases}
    1 & \text{if } Z_{ij} > \tau \\
    0 & \text{else}
  \end{cases}
\end{equation}

The form for the thresholds $\tau$ is still yet to be determined. It will most
likely be derived from the $\mu$ and $\sigma$ to create a threshold $\tau$ that
will allow some but not all maximum values to be considered alignments. We will
investigate by graphing the histograms of true alignment $z$-scores and non $z$-scores and find a good partition. 

\section{Ground Truth Alignment Results}
% ?

\section{Future Work}
% what to get done by final report

%------------------------------------------------------------------------------

% References
\bibliographystyle{abbrv}
\bibliography{interim_report}

%------------------------------------------------------------------------------

\clearpage
\appendix
\onecolumn
\begin{myparindent}{0pt}
\section{Loss Function}
\label{appendix:loss-function}
To create our loss function, we convert our alignment matrix $\psi$ to a
probability distribution. We define $\sigma(\vec{x})$ as the softmax operator
applied on vector $\vec{x}$, and $\sigma(\vec{x})_i$ as the $x_i$ softmax
probability for vector $\vec{x}$.

\begin{equation}
\sigma(\vec{x})_i = \frac{\exp(x_i)}{\sum_j\exp(x_j)}
\end{equation}

We therefore define two operations on the alignment matrix $\psi$. For source
$s$ to target $t$ probability generations, we define $\sigma_t(\psi)$ as the
softmax on each column, i.e., target word $t_j$.
\begin{equation}
  \sigma_t(\psi) = \left[
    \begin{matrix}
      \sigma(\psi_{t_1}) &
      \hdots &
      \sigma(\psi_{t_j}) &
      \hdots &
      \sigma(\psi_{t_m})  \\
    \end{matrix}
\right]
\end{equation}
Equivalently,

\begin{equation}
  \sigma_t(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{kj}}}
\end{equation}

In addition, we further define the target-to-source generation as
$\sigma_s(\psi)$, where the softmax operator is applied on each row for target
word $s_i$ and produces a row vector.
\begin{equation}
  \sigma_s(\psi) = \left[
    \begin{matrix}
      \sigma(\psi_{s_1})  \\
      \vdots \\
      \sigma(\psi_{s_i})  \\
      \vdots \\
      \sigma(\psi_{s_n})  \\
    \end{matrix}
\right]
\end{equation}
Equivalently,

\begin{equation}
  \sigma_s(\psi)_{ij} = \frac{e^{\psi_{ij}}}{\sum_{k} e^{\psi_{ik}}}
\end{equation}

% todo: viv
While variational autoencoders (VAEs) sample from the probability distribution
derived by the neural network, our model's distribution is discrete.
Therefore, we need not sample and can instead compute the loss in terms of all
possibilities.

\subsection{Probability Maximization}

In our original formulation, we maximized probabilities for word alignments.
For this model, we define $p(t, s | \theta, \psi)$ as such:

\begin{equation}
  p(t, s | \theta, \psi)
    = \prod_j^{|t|} \sum_i^{|s|} p_\theta(t_j| s_i) \cdot p_a(i|j)
    = \prod_j^{|t|} \sum_i^{|s|} \sigma_s(\theta(t_j, s_i)) \cdot \sigma_t(\psi)_{ij}
\end{equation}

This formula stems from our assumption that the alignments for each target word
are conditionally independent, hence a product over target words $t_j$. We then
sum over all the alignment possibilities from the source words $s_i$, thus
marginalizing over the alignments.

For $s \mapsto t$ alignments, alignments are softmaxed per column because we
hold the target $t_j$ constant and iterate over the source $s_i$, hence
$\sigma_t$.

The translation probabilities are softmaxed per row (i.e., source word $s_i$)
since we are given a source word and want to know the probability of
translating $s_i$ to $t_j$, hence $\sigma_s$.

However, as we do not maximize in neural networks, we must transform this
equation into a minimization problem by taking the negative log of $p$:

\begin{equation}
  -\log p(t, s | \theta, \psi) =
  - \sum_j^{|t|}
     \log \left[ \sum_i^{|s|} \sigma_s \left( \theta(t_j, s_i) \right) \cdot
      \sigma_t(\psi)_{ij} \right]
\end{equation}

We can simplify this into the equation below, for easier implementation
(via PyTorch's \texttt{logsumexp} function):

\begin{equation}
  -\log  p(t , s | \theta, \psi) =
  - \sum_j^{|t|}  \log \left[ \sum_i^{|s|} \exp
      \left( \log \sigma_s(\theta(t_j, s_i)) + \log \sigma_t(\psi)_{ij} \right)
    \right]
\end{equation}

This is the term to be minimized for our source-to-target word alignment
generation.


\subsection{KL Divergence and Prior Terms}

We also need to minimize the Kullback-Leibler (KL) divergence between our
distribution and a prior. For discrete probability distributions $P$ and $Q$
defined on the same probability space, the reverse KL divergence from $Q$ to
$P$ is defined as:

\begin{equation}
D_{\mathrm{KL}}(Q \| P) = \sum_{i} Q(i) \log \left( \frac{Q(i)}{P(i)} \right)
\end{equation}

In other words, it is the expectation of the log difference between the
probabilities $P$ and $Q$, where the expectation is taken using the
probabilities $Q$.

To calculate the KL divergence of our model, we must define distributions $P$
and $Q$. We first consider $P$ as our prior distribution, henceforth called
$a_t$. This prior distribution $a_t$ is a $n \times m$ matrix filled with the
alignment probabilities filled for source word $s_i$; target word $t_j$; source
sentence $s$ of length $n$; and target sentence $t$ of length $m$:

We define distortion exponent $h$ as:
\begin{equation}
  h(i, j) = {-\lambda \left| \frac{i}{n} - \frac{j}{m}\right|}
\end{equation}

% might have to flip this to be sum over i' not j' DONE
\begin{equation}
  Z_j = \sum_{i'} \exp h(i', j)
\end{equation}

\begin{equation}
a_t (i, j) =
\begin{cases}
      p_0 & \text{if } null \\
     (1-p_0) \cdot \frac{e^{h(i,j)}}{Z_j} & \text{else}
   \end{cases}
\end{equation}

In \cite{dyer2013simple}, parameter values were selected as $\lambda = 4$ and
$p_0 = 0.08$ for the entire corpus. Each element value of $a_t$ is normalized
by the sum of the column distortion values to create a valid distribution via
term $Z_j$ (since we hold target $t_j$ constant). For future notation, let the
subscript on the distribution $a$ denote the way we normalize, i.e., $a_t$ is
the prior alignment distribution normalized with respect to target words $t_j$
and thus normalized by the sum of the column for $t_j$.

% \todo: change?
We can then write our KL Divergence (to be minimized) as:

\begin{equation}
  D_{\mathrm{KL}}(\sigma_t(\psi) \| a_t) =
    \sum_i^n \sum_j^m \sigma_t(\psi)_{ij} \cdot
      \log \left[ \frac{\sigma_t(\psi)_{ij}}{a_t(i, j)} \right]
\end{equation}


\subsection{Target-to-Source Loss Terms}

We have described the loss terms for a source $s$ to target $t$ word alignment.
However, we seek to perform alignment by agreement. Hence, we must add loss
functions that describe the evaluation of target $t$ to source $s$:

\begin{equation}
  -\log  p(t , s | \theta, \psi) =
  - \sum_i^{|s|}  \log \left[ \sum_j^{|t|}
      \exp \left(
        \log \sigma_t(\theta(t_j, s_i)) + \log \sigma_s(\psi)_{ij}
      \right)
    \right]
\end{equation}

\noindent
Additionally, for the KL Divergence term for target $t$ to source $s$:

\begin{equation}
D_{\mathrm{KL}} (\sigma_s(\psi) \| a_s) = \sum_i^n \sum_j^m \sigma_s(\psi)_{ij}
  \cdot \log \left[ \frac{\sigma_s(\psi)_{ij}}{a_s(i, j)} \right]
\end{equation}

\noindent
where the normalization of the prior $a_s$ is on the row instead of the column.
In other words, we hold the source $s_i$ constant; sum the row; and divide each
row element by the sum.

The equations for $a_s$ are as follows:

\begin{equation}
  h(i, j) = {-\lambda \left| \frac{i}{n} - \frac{j}{m}\right|}
\end{equation}

% might have to flip this to be sum over i' not j' DONE
\begin{equation}
  Z_i = \sum_{j'} \exp h(i, j')
\end{equation}

\begin{equation}
a_s (i, j) =
\begin{cases}
      p_0 & \text{if } null \\
     (1-p_0) \cdot \frac{e^{h(i,j)}}{Z_i} & \text{else}
   \end{cases}
\end{equation}

We also perform the softmax operator on each column of the $\psi$ matrix,
previously defined as $\sigma_t(\psi)$.


\subsection{Alignment by Agreement}

Finally, one last loss function term must be added to jointly train each model.
We define $\circ$ as the Hadamard Product, which is the element-wise
multiplication of two matrices. For instance:
$(A \circ B)_{ij} = A_{ij} \cdot B_{ij}$.

We can write the term as such:

\begin{equation}
  -\log \sum_i^{|s|} \sum_j^{|t|}
    \left[ \sigma_s(\psi) \circ \sigma_t(\psi) \right]_{ij}
\end{equation}

% Again note here, you can use logsumexp trick to implement this efficiently in
% PyTorch


\subsection{Combined Loss Function}

Our final loss function, to be minimized, is a 5-term equation. We define:
\begin{itemize}[label={}]
  \item $a_t$ as the target prior alignment matrix normalized per column with respect to $t$\\ %per col wrt t
  \item $a_s$ as the source prior alignment matrix normalized per row with respect to $s$\\ %per row wrt s
  \item $\sigma_s$ as the softmax operator applied on the rows of a matrix\\
  \item $\sigma_t$ as the softmax operator applied to each column of a matrix
\end{itemize}

Indexing is done under the assumption that source words $s_i$ form rows
and target words $t_j$ form columns. The term $ij$ is a shorthand
for $s_i$ and $t_j$ indexing into our matrices.

\begin{equation}
  \centering
\begin{split}
  Lo&ss = \\
  &- \sum_j^{|t|} \log \left[
      \sum_i^{|s|} \exp \left(
        \log \sigma_s(\theta(t, s))_{ij} + \log \sigma_t(\psi)_{ij} \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_t(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_t(\psi)_{ij}}{a_t(i, j)} \right] \\
  &- \sum_i^{|s|} \log \left[ \sum_j^{|t|}
      \exp \left(
        \log \sigma_t(\theta(t, s))_{ij} + \log \sigma_s(\psi)_{ij}
      \right)
    \right] \\
  &+ \sum_i^n \sum_j^m \sigma_s(\psi)_{ij} \cdot \log \left[
    \frac{\sigma_s(\psi)_{ij}}{a_s(i, j)} \right] \\
  &- \log \sum_i^{|s|} \sum_j^{|t|} \left[
    \sigma_s(\psi) \circ \sigma_t(\psi) \right]_{ij} \\
\end{split}
\end{equation}

% Note, the alignment prior is not exactly the same for source-to-target and
% target-to-source because even though the loss function is symmetric, the
% probabilities are normalized with respect to n or m, depending which
% direction you are translating
\end{myparindent}
\end{document}
